{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8849d0d710>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_max = x.max() # 255\n",
    "    return x / x_max\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Initialize a matrix with the apropiate size\n",
    "    one_hot = np.zeros((len(x),10))\n",
    "    \n",
    "    one_hot[np.arange(len(x)), x] = 1\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32,\n",
    "                          shape=[None, image_shape[0],  image_shape[1],  image_shape[2]],\n",
    "                         name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(\n",
    "                tf.float32,\n",
    "                [None, n_classes],\n",
    "                name='y'\n",
    "            )\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return  tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "(4, 4)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "#     print(x_tensor)\n",
    "#     print(conv_num_outputs)\n",
    "    print(conv_ksize)\n",
    "    print(conv_strides)\n",
    "    \n",
    "    input_depth = x_tensor.shape[3]\n",
    "    \n",
    "    shape_array = np.array((conv_ksize[0], conv_ksize[1], input_depth, conv_num_outputs), int)\n",
    "    \n",
    "    weitghs = tf.Variable(tf.truncated_normal(\n",
    "                            shape_array,\n",
    "                            mean=0.0,\n",
    "                            stddev=0.05,\n",
    "                            dtype=tf.float32,\n",
    "                        ))\n",
    "    \n",
    "    biases = tf.Variable(tf.zeros((conv_num_outputs)))\n",
    "    \n",
    "    conv_layer = tf.nn.bias_add(tf.nn.conv2d(\n",
    "                x_tensor,\n",
    "                weitghs,\n",
    "                strides=[1, conv_strides[0], conv_strides[1], 1],\n",
    "                padding='SAME'\n",
    "                ), biases)\n",
    "    \n",
    "    conv_layer_act = tf.nn.relu(conv_layer)\n",
    "    \n",
    "#     print(conv_layer)\n",
    "    # Apply Maxpool\n",
    "    max_pool = tf.nn.max_pool(conv_layer_act, \n",
    "                              ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                              strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "                              padding='SAME'\n",
    "                             )\n",
    "\n",
    "    return max_pool \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    return tf.contrib.layers.flatten(x_tensor)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # Default activation funtion is tf.nn.relu\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs, activation_fn = None)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(2, 2)\n",
      "(4, 4)\n",
      "(2, 2)\n",
      "(4, 4)\n",
      "(2, 2)\n",
      "(4, 4)\n",
      "(2, 2)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 64\n",
    "    conv_ksize = (4, 4)\n",
    "    conv_strides = (2, 2)\n",
    "    pool_ksize  = (2, 2)\n",
    "    pool_strides = (2, 2)\n",
    "    \n",
    "    hidden_conv_1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    hidden_conv_2 = conv2d_maxpool(hidden_conv_1, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flatten_conv = flatten(hidden_conv_2)\n",
    "    flatten_conv = tf.nn.dropout(flatten_conv, keep_prob)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fully_con_1 = fully_conn(flatten_conv, 20)\n",
    "    fully_con_1 = tf.nn.dropout(fully_con_1, keep_prob)\n",
    "\n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(fully_con_1, 10)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: keep_probability\n",
    "    })\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    loss = sess.run(cost, feed_dict={\n",
    "                x: feature_batch,\n",
    "                y: label_batch,\n",
    "                keep_prob: 1.})    \n",
    "    \n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "                x: valid_features,\n",
    "                y: valid_labels,\n",
    "                keep_prob: 1.})\n",
    "    \n",
    "    print(valid_acc)\n",
    "\n",
    "    print('Loss: {:.4f} Validation Accuracy: {:.6f}'.format(\n",
    "                loss,\n",
    "                valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  0.1566\n",
      "Loss: 2.2809 Validation Accuracy: 0.156600\n",
      "Epoch  2, CIFAR-10 Batch 1:  0.176\n",
      "Loss: 2.2325 Validation Accuracy: 0.176000\n",
      "Epoch  3, CIFAR-10 Batch 1:  0.2188\n",
      "Loss: 2.1510 Validation Accuracy: 0.218800\n",
      "Epoch  4, CIFAR-10 Batch 1:  0.2596\n",
      "Loss: 2.0699 Validation Accuracy: 0.259600\n",
      "Epoch  5, CIFAR-10 Batch 1:  0.3182\n",
      "Loss: 2.0105 Validation Accuracy: 0.318200\n",
      "Epoch  6, CIFAR-10 Batch 1:  0.3338\n",
      "Loss: 1.9651 Validation Accuracy: 0.333800\n",
      "Epoch  7, CIFAR-10 Batch 1:  0.3516\n",
      "Loss: 1.9158 Validation Accuracy: 0.351600\n",
      "Epoch  8, CIFAR-10 Batch 1:  0.3704\n",
      "Loss: 1.8802 Validation Accuracy: 0.370400\n",
      "Epoch  9, CIFAR-10 Batch 1:  0.3484\n",
      "Loss: 1.8473 Validation Accuracy: 0.348400\n",
      "Epoch 10, CIFAR-10 Batch 1:  0.3646\n",
      "Loss: 1.8130 Validation Accuracy: 0.364600\n",
      "Epoch 11, CIFAR-10 Batch 1:  0.3906\n",
      "Loss: 1.7716 Validation Accuracy: 0.390600\n",
      "Epoch 12, CIFAR-10 Batch 1:  0.3932\n",
      "Loss: 1.7453 Validation Accuracy: 0.393200\n",
      "Epoch 13, CIFAR-10 Batch 1:  0.4028\n",
      "Loss: 1.7261 Validation Accuracy: 0.402800\n",
      "Epoch 14, CIFAR-10 Batch 1:  0.4124\n",
      "Loss: 1.6998 Validation Accuracy: 0.412400\n",
      "Epoch 15, CIFAR-10 Batch 1:  0.411\n",
      "Loss: 1.6615 Validation Accuracy: 0.411000\n",
      "Epoch 16, CIFAR-10 Batch 1:  0.421\n",
      "Loss: 1.6426 Validation Accuracy: 0.421000\n",
      "Epoch 17, CIFAR-10 Batch 1:  0.4214\n",
      "Loss: 1.6223 Validation Accuracy: 0.421400\n",
      "Epoch 18, CIFAR-10 Batch 1:  0.4296\n",
      "Loss: 1.5986 Validation Accuracy: 0.429600\n",
      "Epoch 19, CIFAR-10 Batch 1:  0.439\n",
      "Loss: 1.5787 Validation Accuracy: 0.439000\n",
      "Epoch 20, CIFAR-10 Batch 1:  0.4456\n",
      "Loss: 1.5513 Validation Accuracy: 0.445600\n",
      "Epoch 21, CIFAR-10 Batch 1:  0.4458\n",
      "Loss: 1.5280 Validation Accuracy: 0.445800\n",
      "Epoch 22, CIFAR-10 Batch 1:  0.441\n",
      "Loss: 1.5202 Validation Accuracy: 0.441000\n",
      "Epoch 23, CIFAR-10 Batch 1:  0.4572\n",
      "Loss: 1.4975 Validation Accuracy: 0.457200\n",
      "Epoch 24, CIFAR-10 Batch 1:  0.4526\n",
      "Loss: 1.4779 Validation Accuracy: 0.452600\n",
      "Epoch 25, CIFAR-10 Batch 1:  0.457\n",
      "Loss: 1.4602 Validation Accuracy: 0.457000\n",
      "Epoch 26, CIFAR-10 Batch 1:  0.4526\n",
      "Loss: 1.4597 Validation Accuracy: 0.452600\n",
      "Epoch 27, CIFAR-10 Batch 1:  0.4706\n",
      "Loss: 1.4470 Validation Accuracy: 0.470600\n",
      "Epoch 28, CIFAR-10 Batch 1:  0.4694\n",
      "Loss: 1.4163 Validation Accuracy: 0.469400\n",
      "Epoch 29, CIFAR-10 Batch 1:  0.4752\n",
      "Loss: 1.4053 Validation Accuracy: 0.475200\n",
      "Epoch 30, CIFAR-10 Batch 1:  0.4526\n",
      "Loss: 1.4190 Validation Accuracy: 0.452600\n",
      "Epoch 31, CIFAR-10 Batch 1:  0.4668\n",
      "Loss: 1.3938 Validation Accuracy: 0.466800\n",
      "Epoch 32, CIFAR-10 Batch 1:  0.465\n",
      "Loss: 1.3845 Validation Accuracy: 0.465000\n",
      "Epoch 33, CIFAR-10 Batch 1:  0.473\n",
      "Loss: 1.3633 Validation Accuracy: 0.473000\n",
      "Epoch 34, CIFAR-10 Batch 1:  0.4752\n",
      "Loss: 1.3447 Validation Accuracy: 0.475200\n",
      "Epoch 35, CIFAR-10 Batch 1:  0.48\n",
      "Loss: 1.3471 Validation Accuracy: 0.480000\n",
      "Epoch 36, CIFAR-10 Batch 1:  0.4734\n",
      "Loss: 1.3204 Validation Accuracy: 0.473400\n",
      "Epoch 37, CIFAR-10 Batch 1:  0.474\n",
      "Loss: 1.3149 Validation Accuracy: 0.474000\n",
      "Epoch 38, CIFAR-10 Batch 1:  0.472\n",
      "Loss: 1.3260 Validation Accuracy: 0.472000\n",
      "Epoch 39, CIFAR-10 Batch 1:  0.4762\n",
      "Loss: 1.3212 Validation Accuracy: 0.476200\n",
      "Epoch 40, CIFAR-10 Batch 1:  0.4814\n",
      "Loss: 1.3046 Validation Accuracy: 0.481400\n",
      "Epoch 41, CIFAR-10 Batch 1:  0.4876\n",
      "Loss: 1.2711 Validation Accuracy: 0.487600\n",
      "Epoch 42, CIFAR-10 Batch 1:  0.4974\n",
      "Loss: 1.2750 Validation Accuracy: 0.497400\n",
      "Epoch 43, CIFAR-10 Batch 1:  0.4902\n",
      "Loss: 1.2669 Validation Accuracy: 0.490200\n",
      "Epoch 44, CIFAR-10 Batch 1:  0.4866\n",
      "Loss: 1.2626 Validation Accuracy: 0.486600\n",
      "Epoch 45, CIFAR-10 Batch 1:  0.488\n",
      "Loss: 1.2427 Validation Accuracy: 0.488000\n",
      "Epoch 46, CIFAR-10 Batch 1:  0.4832\n",
      "Loss: 1.2501 Validation Accuracy: 0.483200\n",
      "Epoch 47, CIFAR-10 Batch 1:  0.4892\n",
      "Loss: 1.2296 Validation Accuracy: 0.489200\n",
      "Epoch 48, CIFAR-10 Batch 1:  0.4914\n",
      "Loss: 1.2103 Validation Accuracy: 0.491400\n",
      "Epoch 49, CIFAR-10 Batch 1:  0.507\n",
      "Loss: 1.1958 Validation Accuracy: 0.507000\n",
      "Epoch 50, CIFAR-10 Batch 1:  0.5082\n",
      "Loss: 1.1939 Validation Accuracy: 0.508200\n",
      "Epoch 51, CIFAR-10 Batch 1:  0.5124\n",
      "Loss: 1.1821 Validation Accuracy: 0.512400\n",
      "Epoch 52, CIFAR-10 Batch 1:  0.5058\n",
      "Loss: 1.1804 Validation Accuracy: 0.505800\n",
      "Epoch 53, CIFAR-10 Batch 1:  0.5122\n",
      "Loss: 1.1649 Validation Accuracy: 0.512200\n",
      "Epoch 54, CIFAR-10 Batch 1:  0.519\n",
      "Loss: 1.1543 Validation Accuracy: 0.519000\n",
      "Epoch 55, CIFAR-10 Batch 1:  0.5128\n",
      "Loss: 1.1412 Validation Accuracy: 0.512800\n",
      "Epoch 56, CIFAR-10 Batch 1:  0.5142\n",
      "Loss: 1.1392 Validation Accuracy: 0.514200\n",
      "Epoch 57, CIFAR-10 Batch 1:  0.5182\n",
      "Loss: 1.1181 Validation Accuracy: 0.518200\n",
      "Epoch 58, CIFAR-10 Batch 1:  0.5154\n",
      "Loss: 1.1100 Validation Accuracy: 0.515400\n",
      "Epoch 59, CIFAR-10 Batch 1:  0.5152\n",
      "Loss: 1.0979 Validation Accuracy: 0.515200\n",
      "Epoch 60, CIFAR-10 Batch 1:  0.528\n",
      "Loss: 1.1005 Validation Accuracy: 0.528000\n",
      "Epoch 61, CIFAR-10 Batch 1:  0.5238\n",
      "Loss: 1.0888 Validation Accuracy: 0.523800\n",
      "Epoch 62, CIFAR-10 Batch 1:  0.5146\n",
      "Loss: 1.0962 Validation Accuracy: 0.514600\n",
      "Epoch 63, CIFAR-10 Batch 1:  0.5216\n",
      "Loss: 1.0742 Validation Accuracy: 0.521600\n",
      "Epoch 64, CIFAR-10 Batch 1:  0.5282\n",
      "Loss: 1.0791 Validation Accuracy: 0.528200\n",
      "Epoch 65, CIFAR-10 Batch 1:  0.5226\n",
      "Loss: 1.0607 Validation Accuracy: 0.522600\n",
      "Epoch 66, CIFAR-10 Batch 1:  0.5214\n",
      "Loss: 1.0513 Validation Accuracy: 0.521400\n",
      "Epoch 67, CIFAR-10 Batch 1:  0.529\n",
      "Loss: 1.0528 Validation Accuracy: 0.529000\n",
      "Epoch 68, CIFAR-10 Batch 1:  0.5314\n",
      "Loss: 1.0390 Validation Accuracy: 0.531400\n",
      "Epoch 69, CIFAR-10 Batch 1:  0.5262\n",
      "Loss: 1.0256 Validation Accuracy: 0.526200\n",
      "Epoch 70, CIFAR-10 Batch 1:  0.5302\n",
      "Loss: 1.0282 Validation Accuracy: 0.530200\n",
      "Epoch 71, CIFAR-10 Batch 1:  0.53\n",
      "Loss: 1.0137 Validation Accuracy: 0.530000\n",
      "Epoch 72, CIFAR-10 Batch 1:  0.5254\n",
      "Loss: 1.0178 Validation Accuracy: 0.525400\n",
      "Epoch 73, CIFAR-10 Batch 1:  0.5324\n",
      "Loss: 0.9948 Validation Accuracy: 0.532400\n",
      "Epoch 74, CIFAR-10 Batch 1:  0.5318\n",
      "Loss: 1.0014 Validation Accuracy: 0.531800\n",
      "Epoch 75, CIFAR-10 Batch 1:  0.5376\n",
      "Loss: 1.0021 Validation Accuracy: 0.537600\n",
      "Epoch 76, CIFAR-10 Batch 1:  0.5386\n",
      "Loss: 1.0000 Validation Accuracy: 0.538600\n",
      "Epoch 77, CIFAR-10 Batch 1:  0.5394\n",
      "Loss: 0.9867 Validation Accuracy: 0.539400\n",
      "Epoch 78, CIFAR-10 Batch 1:  0.5274\n",
      "Loss: 0.9755 Validation Accuracy: 0.527400\n",
      "Epoch 79, CIFAR-10 Batch 1:  0.5414\n",
      "Loss: 0.9550 Validation Accuracy: 0.541400\n",
      "Epoch 80, CIFAR-10 Batch 1:  0.5388\n",
      "Loss: 0.9453 Validation Accuracy: 0.538800\n",
      "Epoch 81, CIFAR-10 Batch 1:  0.5414\n",
      "Loss: 0.9382 Validation Accuracy: 0.541400\n",
      "Epoch 82, CIFAR-10 Batch 1:  0.538\n",
      "Loss: 0.9422 Validation Accuracy: 0.538000\n",
      "Epoch 83, CIFAR-10 Batch 1:  0.5326\n",
      "Loss: 0.9542 Validation Accuracy: 0.532600\n",
      "Epoch 84, CIFAR-10 Batch 1:  0.54\n",
      "Loss: 0.9310 Validation Accuracy: 0.540000\n",
      "Epoch 85, CIFAR-10 Batch 1:  0.546\n",
      "Loss: 0.9250 Validation Accuracy: 0.546000\n",
      "Epoch 86, CIFAR-10 Batch 1:  0.5484\n",
      "Loss: 0.9161 Validation Accuracy: 0.548400\n",
      "Epoch 87, CIFAR-10 Batch 1:  0.542\n",
      "Loss: 0.9190 Validation Accuracy: 0.542000\n",
      "Epoch 88, CIFAR-10 Batch 1:  0.5438\n",
      "Loss: 0.9009 Validation Accuracy: 0.543800\n",
      "Epoch 89, CIFAR-10 Batch 1:  0.542\n",
      "Loss: 0.8928 Validation Accuracy: 0.542000\n",
      "Epoch 90, CIFAR-10 Batch 1:  0.5366\n",
      "Loss: 0.9097 Validation Accuracy: 0.536600\n",
      "Epoch 91, CIFAR-10 Batch 1:  0.5432\n",
      "Loss: 0.8950 Validation Accuracy: 0.543200\n",
      "Epoch 92, CIFAR-10 Batch 1:  0.5398\n",
      "Loss: 0.8935 Validation Accuracy: 0.539800\n",
      "Epoch 93, CIFAR-10 Batch 1:  0.5312\n",
      "Loss: 0.9024 Validation Accuracy: 0.531200\n",
      "Epoch 94, CIFAR-10 Batch 1:  0.5546\n",
      "Loss: 0.8708 Validation Accuracy: 0.554600\n",
      "Epoch 95, CIFAR-10 Batch 1:  0.5548\n",
      "Loss: 0.8531 Validation Accuracy: 0.554800\n",
      "Epoch 96, CIFAR-10 Batch 1:  0.5552\n",
      "Loss: 0.8387 Validation Accuracy: 0.555200\n",
      "Epoch 97, CIFAR-10 Batch 1:  0.5514\n",
      "Loss: 0.8523 Validation Accuracy: 0.551400\n",
      "Epoch 98, CIFAR-10 Batch 1:  0.5472\n",
      "Loss: 0.8406 Validation Accuracy: 0.547200\n",
      "Epoch 99, CIFAR-10 Batch 1:  0.5564\n",
      "Loss: 0.8288 Validation Accuracy: 0.556400\n",
      "Epoch 100, CIFAR-10 Batch 1:  0.5558\n",
      "Loss: 0.8222 Validation Accuracy: 0.555800\n",
      "Epoch 101, CIFAR-10 Batch 1:  0.5488\n",
      "Loss: 0.8210 Validation Accuracy: 0.548800\n",
      "Epoch 102, CIFAR-10 Batch 1:  0.5548\n",
      "Loss: 0.8119 Validation Accuracy: 0.554800\n",
      "Epoch 103, CIFAR-10 Batch 1:  0.5536\n",
      "Loss: 0.8203 Validation Accuracy: 0.553600\n",
      "Epoch 104, CIFAR-10 Batch 1:  0.552\n",
      "Loss: 0.8238 Validation Accuracy: 0.552000\n",
      "Epoch 105, CIFAR-10 Batch 1:  0.5522\n",
      "Loss: 0.8211 Validation Accuracy: 0.552200\n",
      "Epoch 106, CIFAR-10 Batch 1:  0.5572\n",
      "Loss: 0.7977 Validation Accuracy: 0.557200\n",
      "Epoch 107, CIFAR-10 Batch 1:  0.5438\n",
      "Loss: 0.8045 Validation Accuracy: 0.543800\n",
      "Epoch 108, CIFAR-10 Batch 1:  0.5578\n",
      "Loss: 0.7904 Validation Accuracy: 0.557800\n",
      "Epoch 109, CIFAR-10 Batch 1:  0.559\n",
      "Loss: 0.7753 Validation Accuracy: 0.559000\n",
      "Epoch 110, CIFAR-10 Batch 1:  0.5542\n",
      "Loss: 0.7773 Validation Accuracy: 0.554200\n",
      "Epoch 111, CIFAR-10 Batch 1:  0.5544\n",
      "Loss: 0.7827 Validation Accuracy: 0.554400\n",
      "Epoch 112, CIFAR-10 Batch 1:  0.555\n",
      "Loss: 0.7619 Validation Accuracy: 0.555000\n",
      "Epoch 113, CIFAR-10 Batch 1:  0.5512\n",
      "Loss: 0.7635 Validation Accuracy: 0.551200\n",
      "Epoch 114, CIFAR-10 Batch 1:  0.5592\n",
      "Loss: 0.7660 Validation Accuracy: 0.559200\n",
      "Epoch 115, CIFAR-10 Batch 1:  0.5542\n",
      "Loss: 0.7555 Validation Accuracy: 0.554200\n",
      "Epoch 116, CIFAR-10 Batch 1:  0.5558\n",
      "Loss: 0.7674 Validation Accuracy: 0.555800\n",
      "Epoch 117, CIFAR-10 Batch 1:  0.562\n",
      "Loss: 0.7516 Validation Accuracy: 0.562000\n",
      "Epoch 118, CIFAR-10 Batch 1:  0.5564\n",
      "Loss: 0.7636 Validation Accuracy: 0.556400\n",
      "Epoch 119, CIFAR-10 Batch 1:  0.5516\n",
      "Loss: 0.7540 Validation Accuracy: 0.551600\n",
      "Epoch 120, CIFAR-10 Batch 1:  0.5586\n",
      "Loss: 0.7476 Validation Accuracy: 0.558600\n",
      "Epoch 121, CIFAR-10 Batch 1:  0.563\n",
      "Loss: 0.7417 Validation Accuracy: 0.563000\n",
      "Epoch 122, CIFAR-10 Batch 1:  0.5574\n",
      "Loss: 0.7327 Validation Accuracy: 0.557400\n",
      "Epoch 123, CIFAR-10 Batch 1:  0.562\n",
      "Loss: 0.7259 Validation Accuracy: 0.562000\n",
      "Epoch 124, CIFAR-10 Batch 1:  0.5548\n",
      "Loss: 0.7253 Validation Accuracy: 0.554800\n",
      "Epoch 125, CIFAR-10 Batch 1:  0.5506\n",
      "Loss: 0.7332 Validation Accuracy: 0.550600\n",
      "Epoch 126, CIFAR-10 Batch 1:  0.559\n",
      "Loss: 0.7334 Validation Accuracy: 0.559000\n",
      "Epoch 127, CIFAR-10 Batch 1:  0.5542\n",
      "Loss: 0.7287 Validation Accuracy: 0.554200\n",
      "Epoch 128, CIFAR-10 Batch 1:  0.5556\n",
      "Loss: 0.7095 Validation Accuracy: 0.555600\n",
      "Epoch 129, CIFAR-10 Batch 1:  0.5566\n",
      "Loss: 0.7240 Validation Accuracy: 0.556600\n",
      "Epoch 130, CIFAR-10 Batch 1:  0.5646\n",
      "Loss: 0.7071 Validation Accuracy: 0.564600\n",
      "Epoch 131, CIFAR-10 Batch 1:  0.5612\n",
      "Loss: 0.6834 Validation Accuracy: 0.561200\n",
      "Epoch 132, CIFAR-10 Batch 1:  0.561\n",
      "Loss: 0.6932 Validation Accuracy: 0.561000\n",
      "Epoch 133, CIFAR-10 Batch 1:  0.5562\n",
      "Loss: 0.6978 Validation Accuracy: 0.556200\n",
      "Epoch 134, CIFAR-10 Batch 1:  0.5436\n",
      "Loss: 0.7098 Validation Accuracy: 0.543600\n",
      "Epoch 135, CIFAR-10 Batch 1:  0.557\n",
      "Loss: 0.6974 Validation Accuracy: 0.557000\n",
      "Epoch 136, CIFAR-10 Batch 1:  0.563\n",
      "Loss: 0.6814 Validation Accuracy: 0.563000\n",
      "Epoch 137, CIFAR-10 Batch 1:  0.5622\n",
      "Loss: 0.6667 Validation Accuracy: 0.562200\n",
      "Epoch 138, CIFAR-10 Batch 1:  0.5486\n",
      "Loss: 0.6898 Validation Accuracy: 0.548600\n",
      "Epoch 139, CIFAR-10 Batch 1:  0.5634\n",
      "Loss: 0.6628 Validation Accuracy: 0.563400\n",
      "Epoch 140, CIFAR-10 Batch 1:  0.5568\n",
      "Loss: 0.6655 Validation Accuracy: 0.556800\n",
      "Epoch 141, CIFAR-10 Batch 1:  0.5574\n",
      "Loss: 0.6523 Validation Accuracy: 0.557400\n",
      "Epoch 142, CIFAR-10 Batch 1:  0.559\n",
      "Loss: 0.6467 Validation Accuracy: 0.559000\n",
      "Epoch 143, CIFAR-10 Batch 1:  0.5528\n",
      "Loss: 0.6597 Validation Accuracy: 0.552800\n",
      "Epoch 144, CIFAR-10 Batch 1:  0.5652\n",
      "Loss: 0.6431 Validation Accuracy: 0.565200\n",
      "Epoch 145, CIFAR-10 Batch 1:  0.5616\n",
      "Loss: 0.6466 Validation Accuracy: 0.561600\n",
      "Epoch 146, CIFAR-10 Batch 1:  0.5618\n",
      "Loss: 0.6296 Validation Accuracy: 0.561800\n",
      "Epoch 147, CIFAR-10 Batch 1:  0.5564\n",
      "Loss: 0.6439 Validation Accuracy: 0.556400\n",
      "Epoch 148, CIFAR-10 Batch 1:  0.5656\n",
      "Loss: 0.6308 Validation Accuracy: 0.565600\n",
      "Epoch 149, CIFAR-10 Batch 1:  0.564\n",
      "Loss: 0.6386 Validation Accuracy: 0.564000\n",
      "Epoch 150, CIFAR-10 Batch 1:  0.5598\n",
      "Loss: 0.6344 Validation Accuracy: 0.559800\n",
      "Epoch 151, CIFAR-10 Batch 1:  0.5556\n",
      "Loss: 0.6256 Validation Accuracy: 0.555600\n",
      "Epoch 152, CIFAR-10 Batch 1:  0.5486\n",
      "Loss: 0.6419 Validation Accuracy: 0.548600\n",
      "Epoch 153, CIFAR-10 Batch 1:  0.558\n",
      "Loss: 0.6278 Validation Accuracy: 0.558000\n",
      "Epoch 154, CIFAR-10 Batch 1:  0.5564\n",
      "Loss: 0.6216 Validation Accuracy: 0.556400\n",
      "Epoch 155, CIFAR-10 Batch 1:  0.5542\n",
      "Loss: 0.6012 Validation Accuracy: 0.554200\n",
      "Epoch 156, CIFAR-10 Batch 1:  0.5634\n",
      "Loss: 0.6058 Validation Accuracy: 0.563400\n",
      "Epoch 157, CIFAR-10 Batch 1:  0.5538\n",
      "Loss: 0.6225 Validation Accuracy: 0.553800\n",
      "Epoch 158, CIFAR-10 Batch 1:  0.5626\n",
      "Loss: 0.6011 Validation Accuracy: 0.562600\n",
      "Epoch 159, CIFAR-10 Batch 1:  0.5606\n",
      "Loss: 0.6094 Validation Accuracy: 0.560600\n",
      "Epoch 160, CIFAR-10 Batch 1:  0.565\n",
      "Loss: 0.5919 Validation Accuracy: 0.565000\n",
      "Epoch 161, CIFAR-10 Batch 1:  0.5652\n",
      "Loss: 0.5896 Validation Accuracy: 0.565200\n",
      "Epoch 162, CIFAR-10 Batch 1:  0.5652\n",
      "Loss: 0.5988 Validation Accuracy: 0.565200\n",
      "Epoch 163, CIFAR-10 Batch 1:  0.5724\n",
      "Loss: 0.5876 Validation Accuracy: 0.572400\n",
      "Epoch 164, CIFAR-10 Batch 1:  0.5644\n",
      "Loss: 0.5924 Validation Accuracy: 0.564400\n",
      "Epoch 165, CIFAR-10 Batch 1:  0.566\n",
      "Loss: 0.5850 Validation Accuracy: 0.566000\n",
      "Epoch 166, CIFAR-10 Batch 1:  0.5656\n",
      "Loss: 0.5708 Validation Accuracy: 0.565600\n",
      "Epoch 167, CIFAR-10 Batch 1:  0.5584\n",
      "Loss: 0.5729 Validation Accuracy: 0.558400\n",
      "Epoch 168, CIFAR-10 Batch 1:  0.5606\n",
      "Loss: 0.5804 Validation Accuracy: 0.560600\n",
      "Epoch 169, CIFAR-10 Batch 1:  0.5598\n",
      "Loss: 0.5775 Validation Accuracy: 0.559800\n",
      "Epoch 170, CIFAR-10 Batch 1:  0.5624\n",
      "Loss: 0.5689 Validation Accuracy: 0.562400\n",
      "Epoch 171, CIFAR-10 Batch 1:  0.5646\n",
      "Loss: 0.5598 Validation Accuracy: 0.564600\n",
      "Epoch 172, CIFAR-10 Batch 1:  0.5628\n",
      "Loss: 0.5639 Validation Accuracy: 0.562800\n",
      "Epoch 173, CIFAR-10 Batch 1:  0.559\n",
      "Loss: 0.5580 Validation Accuracy: 0.559000\n",
      "Epoch 174, CIFAR-10 Batch 1:  0.5596\n",
      "Loss: 0.5711 Validation Accuracy: 0.559600\n",
      "Epoch 175, CIFAR-10 Batch 1:  0.5606\n",
      "Loss: 0.5797 Validation Accuracy: 0.560600\n",
      "Epoch 176, CIFAR-10 Batch 1:  0.5584\n",
      "Loss: 0.5518 Validation Accuracy: 0.558400\n",
      "Epoch 177, CIFAR-10 Batch 1:  0.5588\n",
      "Loss: 0.5589 Validation Accuracy: 0.558800\n",
      "Epoch 178, CIFAR-10 Batch 1:  0.5616\n",
      "Loss: 0.5534 Validation Accuracy: 0.561600\n",
      "Epoch 179, CIFAR-10 Batch 1:  0.5692\n",
      "Loss: 0.5394 Validation Accuracy: 0.569200\n",
      "Epoch 180, CIFAR-10 Batch 1:  0.5654\n",
      "Loss: 0.5354 Validation Accuracy: 0.565400\n",
      "Epoch 181, CIFAR-10 Batch 1:  0.565\n",
      "Loss: 0.5374 Validation Accuracy: 0.565000\n",
      "Epoch 182, CIFAR-10 Batch 1:  0.563\n",
      "Loss: 0.5392 Validation Accuracy: 0.563000\n",
      "Epoch 183, CIFAR-10 Batch 1:  0.5598\n",
      "Loss: 0.5442 Validation Accuracy: 0.559800\n",
      "Epoch 184, CIFAR-10 Batch 1:  0.5654\n",
      "Loss: 0.5451 Validation Accuracy: 0.565400\n",
      "Epoch 185, CIFAR-10 Batch 1:  0.56\n",
      "Loss: 0.5370 Validation Accuracy: 0.560000\n",
      "Epoch 186, CIFAR-10 Batch 1:  0.5614\n",
      "Loss: 0.5432 Validation Accuracy: 0.561400\n",
      "Epoch 187, CIFAR-10 Batch 1:  0.56\n",
      "Loss: 0.5417 Validation Accuracy: 0.560000\n",
      "Epoch 188, CIFAR-10 Batch 1:  0.5508\n",
      "Loss: 0.5583 Validation Accuracy: 0.550800\n",
      "Epoch 189, CIFAR-10 Batch 1:  0.5662\n",
      "Loss: 0.5313 Validation Accuracy: 0.566200\n",
      "Epoch 190, CIFAR-10 Batch 1:  0.559\n",
      "Loss: 0.5396 Validation Accuracy: 0.559000\n",
      "Epoch 191, CIFAR-10 Batch 1:  0.5638\n",
      "Loss: 0.5398 Validation Accuracy: 0.563800\n",
      "Epoch 192, CIFAR-10 Batch 1:  0.5582\n",
      "Loss: 0.5185 Validation Accuracy: 0.558200\n",
      "Epoch 193, CIFAR-10 Batch 1:  0.5598\n",
      "Loss: 0.5075 Validation Accuracy: 0.559800\n",
      "Epoch 194, CIFAR-10 Batch 1:  0.5618\n",
      "Loss: 0.4975 Validation Accuracy: 0.561800\n",
      "Epoch 195, CIFAR-10 Batch 1:  0.5616\n",
      "Loss: 0.5122 Validation Accuracy: 0.561600\n",
      "Epoch 196, CIFAR-10 Batch 1:  0.5582\n",
      "Loss: 0.5237 Validation Accuracy: 0.558200\n",
      "Epoch 197, CIFAR-10 Batch 1:  0.5562\n",
      "Loss: 0.5220 Validation Accuracy: 0.556200\n",
      "Epoch 198, CIFAR-10 Batch 1:  0.5588\n",
      "Loss: 0.5037 Validation Accuracy: 0.558800\n",
      "Epoch 199, CIFAR-10 Batch 1:  0.557\n",
      "Loss: 0.5110 Validation Accuracy: 0.557000\n",
      "Epoch 200, CIFAR-10 Batch 1:  0.5632\n",
      "Loss: 0.4938 Validation Accuracy: 0.563200\n",
      "CPU times: user 2min 12s, sys: 1min 3s, total: 3min 15s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  0.1096\n",
      "Loss: 2.3013 Validation Accuracy: 0.109600\n",
      "Epoch  1, CIFAR-10 Batch 2:  0.1454\n",
      "Loss: 2.2872 Validation Accuracy: 0.145400\n",
      "Epoch  1, CIFAR-10 Batch 3:  0.189\n",
      "Loss: 2.2566 Validation Accuracy: 0.189000\n",
      "Epoch  1, CIFAR-10 Batch 4:  0.2242\n",
      "Loss: 2.1569 Validation Accuracy: 0.224200\n",
      "Epoch  1, CIFAR-10 Batch 5:  0.2612\n",
      "Loss: 2.0812 Validation Accuracy: 0.261200\n",
      "Epoch  2, CIFAR-10 Batch 1:  0.3002\n",
      "Loss: 2.0514 Validation Accuracy: 0.300200\n",
      "Epoch  2, CIFAR-10 Batch 2:  0.329\n",
      "Loss: 1.9731 Validation Accuracy: 0.329000\n",
      "Epoch  2, CIFAR-10 Batch 3:  0.3248\n",
      "Loss: 1.8635 Validation Accuracy: 0.324800\n",
      "Epoch  2, CIFAR-10 Batch 4:  0.349\n",
      "Loss: 1.8006 Validation Accuracy: 0.349000\n",
      "Epoch  2, CIFAR-10 Batch 5:  0.3688\n",
      "Loss: 1.8107 Validation Accuracy: 0.368800\n",
      "Epoch  3, CIFAR-10 Batch 1:  0.3858\n",
      "Loss: 1.8256 Validation Accuracy: 0.385800\n",
      "Epoch  3, CIFAR-10 Batch 2:  0.3948\n",
      "Loss: 1.7819 Validation Accuracy: 0.394800\n",
      "Epoch  3, CIFAR-10 Batch 3:  0.4018\n",
      "Loss: 1.6279 Validation Accuracy: 0.401800\n",
      "Epoch  3, CIFAR-10 Batch 4:  0.3964\n",
      "Loss: 1.6386 Validation Accuracy: 0.396400\n",
      "Epoch  3, CIFAR-10 Batch 5:  0.4024\n",
      "Loss: 1.6877 Validation Accuracy: 0.402400\n",
      "Epoch  4, CIFAR-10 Batch 1:  0.4024\n",
      "Loss: 1.7093 Validation Accuracy: 0.402400\n",
      "Epoch  4, CIFAR-10 Batch 2:  0.4282\n",
      "Loss: 1.6796 Validation Accuracy: 0.428200\n",
      "Epoch  4, CIFAR-10 Batch 3:  0.4348\n",
      "Loss: 1.5377 Validation Accuracy: 0.434800\n",
      "Epoch  4, CIFAR-10 Batch 4:  0.4328\n",
      "Loss: 1.5432 Validation Accuracy: 0.432800\n",
      "Epoch  4, CIFAR-10 Batch 5:  0.4292\n",
      "Loss: 1.5784 Validation Accuracy: 0.429200\n",
      "Epoch  5, CIFAR-10 Batch 1:  0.43\n",
      "Loss: 1.6321 Validation Accuracy: 0.430000\n",
      "Epoch  5, CIFAR-10 Batch 2:  0.4484\n",
      "Loss: 1.6084 Validation Accuracy: 0.448400\n",
      "Epoch  5, CIFAR-10 Batch 3:  0.4566\n",
      "Loss: 1.4526 Validation Accuracy: 0.456600\n",
      "Epoch  5, CIFAR-10 Batch 4:  0.441\n",
      "Loss: 1.4632 Validation Accuracy: 0.441000\n",
      "Epoch  5, CIFAR-10 Batch 5:  0.446\n",
      "Loss: 1.5353 Validation Accuracy: 0.446000\n",
      "Epoch  6, CIFAR-10 Batch 1:  0.4556\n",
      "Loss: 1.5814 Validation Accuracy: 0.455600\n",
      "Epoch  6, CIFAR-10 Batch 2:  0.4644\n",
      "Loss: 1.5277 Validation Accuracy: 0.464400\n",
      "Epoch  6, CIFAR-10 Batch 3:  0.4696\n",
      "Loss: 1.3859 Validation Accuracy: 0.469600\n",
      "Epoch  6, CIFAR-10 Batch 4:  0.4592\n",
      "Loss: 1.4087 Validation Accuracy: 0.459200\n",
      "Epoch  6, CIFAR-10 Batch 5:  0.465\n",
      "Loss: 1.4747 Validation Accuracy: 0.465000\n",
      "Epoch  7, CIFAR-10 Batch 1:  0.4672\n",
      "Loss: 1.5476 Validation Accuracy: 0.467200\n",
      "Epoch  7, CIFAR-10 Batch 2:  0.4766\n",
      "Loss: 1.4975 Validation Accuracy: 0.476600\n",
      "Epoch  7, CIFAR-10 Batch 3:  0.4662\n",
      "Loss: 1.3479 Validation Accuracy: 0.466200\n",
      "Epoch  7, CIFAR-10 Batch 4:  0.4682\n",
      "Loss: 1.3850 Validation Accuracy: 0.468200\n",
      "Epoch  7, CIFAR-10 Batch 5:  0.483\n",
      "Loss: 1.4444 Validation Accuracy: 0.483000\n",
      "Epoch  8, CIFAR-10 Batch 1:  0.491\n",
      "Loss: 1.4891 Validation Accuracy: 0.491000\n",
      "Epoch  8, CIFAR-10 Batch 2:  0.488\n",
      "Loss: 1.4383 Validation Accuracy: 0.488000\n",
      "Epoch  8, CIFAR-10 Batch 3:  0.4788\n",
      "Loss: 1.3115 Validation Accuracy: 0.478800\n",
      "Epoch  8, CIFAR-10 Batch 4:  0.4886\n",
      "Loss: 1.3347 Validation Accuracy: 0.488600\n",
      "Epoch  8, CIFAR-10 Batch 5:  0.4832\n",
      "Loss: 1.4058 Validation Accuracy: 0.483200\n",
      "Epoch  9, CIFAR-10 Batch 1:  0.4948\n",
      "Loss: 1.4592 Validation Accuracy: 0.494800\n",
      "Epoch  9, CIFAR-10 Batch 2:  0.4992\n",
      "Loss: 1.4038 Validation Accuracy: 0.499200\n",
      "Epoch  9, CIFAR-10 Batch 3:  0.485\n",
      "Loss: 1.2781 Validation Accuracy: 0.485000\n",
      "Epoch  9, CIFAR-10 Batch 4:  0.498\n",
      "Loss: 1.3002 Validation Accuracy: 0.498000\n",
      "Epoch  9, CIFAR-10 Batch 5:  0.5016\n",
      "Loss: 1.3738 Validation Accuracy: 0.501600\n",
      "Epoch 10, CIFAR-10 Batch 1:  0.4654\n",
      "Loss: 1.4679 Validation Accuracy: 0.465400\n",
      "Epoch 10, CIFAR-10 Batch 2:  0.5058\n",
      "Loss: 1.3676 Validation Accuracy: 0.505800\n",
      "Epoch 10, CIFAR-10 Batch 3:  0.512\n",
      "Loss: 1.2156 Validation Accuracy: 0.512000\n",
      "Epoch 10, CIFAR-10 Batch 4:  0.5002\n",
      "Loss: 1.2834 Validation Accuracy: 0.500200\n",
      "Epoch 10, CIFAR-10 Batch 5:  0.5002\n",
      "Loss: 1.3763 Validation Accuracy: 0.500200\n",
      "Epoch 11, CIFAR-10 Batch 1:  0.5056\n",
      "Loss: 1.3916 Validation Accuracy: 0.505600\n",
      "Epoch 11, CIFAR-10 Batch 2:  0.5162\n",
      "Loss: 1.3353 Validation Accuracy: 0.516200\n",
      "Epoch 11, CIFAR-10 Batch 3:  0.517\n",
      "Loss: 1.2052 Validation Accuracy: 0.517000\n",
      "Epoch 11, CIFAR-10 Batch 4:  0.5162\n",
      "Loss: 1.2454 Validation Accuracy: 0.516200\n",
      "Epoch 11, CIFAR-10 Batch 5:  0.5176\n",
      "Loss: 1.3104 Validation Accuracy: 0.517600\n",
      "Epoch 12, CIFAR-10 Batch 1:  0.5052\n",
      "Loss: 1.3921 Validation Accuracy: 0.505200\n",
      "Epoch 12, CIFAR-10 Batch 2:  0.515\n",
      "Loss: 1.3365 Validation Accuracy: 0.515000\n",
      "Epoch 12, CIFAR-10 Batch 3:  0.527\n",
      "Loss: 1.1581 Validation Accuracy: 0.527000\n",
      "Epoch 12, CIFAR-10 Batch 4:  0.5266\n",
      "Loss: 1.2210 Validation Accuracy: 0.526600\n",
      "Epoch 12, CIFAR-10 Batch 5:  0.5256\n",
      "Loss: 1.2917 Validation Accuracy: 0.525600\n",
      "Epoch 13, CIFAR-10 Batch 1:  0.5234\n",
      "Loss: 1.3596 Validation Accuracy: 0.523400\n",
      "Epoch 13, CIFAR-10 Batch 2:  0.524\n",
      "Loss: 1.2984 Validation Accuracy: 0.524000\n",
      "Epoch 13, CIFAR-10 Batch 3:  0.5326\n",
      "Loss: 1.1541 Validation Accuracy: 0.532600\n",
      "Epoch 13, CIFAR-10 Batch 4:  0.5292\n",
      "Loss: 1.2006 Validation Accuracy: 0.529200\n",
      "Epoch 13, CIFAR-10 Batch 5:  0.5198\n",
      "Loss: 1.2828 Validation Accuracy: 0.519800\n",
      "Epoch 14, CIFAR-10 Batch 1:  0.5138\n",
      "Loss: 1.3471 Validation Accuracy: 0.513800\n",
      "Epoch 14, CIFAR-10 Batch 2:  0.5312\n",
      "Loss: 1.2813 Validation Accuracy: 0.531200\n",
      "Epoch 14, CIFAR-10 Batch 3:  0.5352\n",
      "Loss: 1.1431 Validation Accuracy: 0.535200\n",
      "Epoch 14, CIFAR-10 Batch 4:  0.5304\n",
      "Loss: 1.1748 Validation Accuracy: 0.530400\n",
      "Epoch 14, CIFAR-10 Batch 5:  0.5346\n",
      "Loss: 1.2402 Validation Accuracy: 0.534600\n",
      "Epoch 15, CIFAR-10 Batch 1:  0.5172\n",
      "Loss: 1.3194 Validation Accuracy: 0.517200\n",
      "Epoch 15, CIFAR-10 Batch 2:  0.5416\n",
      "Loss: 1.2647 Validation Accuracy: 0.541600\n",
      "Epoch 15, CIFAR-10 Batch 3:  0.5402\n",
      "Loss: 1.0853 Validation Accuracy: 0.540200\n",
      "Epoch 15, CIFAR-10 Batch 4:  0.5402\n",
      "Loss: 1.1489 Validation Accuracy: 0.540200\n",
      "Epoch 15, CIFAR-10 Batch 5:  0.549\n",
      "Loss: 1.2176 Validation Accuracy: 0.549000\n",
      "Epoch 16, CIFAR-10 Batch 1:  0.5312\n",
      "Loss: 1.2926 Validation Accuracy: 0.531200\n",
      "Epoch 16, CIFAR-10 Batch 2:  0.5438\n",
      "Loss: 1.2303 Validation Accuracy: 0.543800\n",
      "Epoch 16, CIFAR-10 Batch 3:  0.5452\n",
      "Loss: 1.1058 Validation Accuracy: 0.545200\n",
      "Epoch 16, CIFAR-10 Batch 4:  0.5456\n",
      "Loss: 1.1475 Validation Accuracy: 0.545600\n",
      "Epoch 16, CIFAR-10 Batch 5:  0.5346\n",
      "Loss: 1.2188 Validation Accuracy: 0.534600\n",
      "Epoch 17, CIFAR-10 Batch 1:  0.5396\n",
      "Loss: 1.2592 Validation Accuracy: 0.539600\n",
      "Epoch 17, CIFAR-10 Batch 2:  0.5384\n",
      "Loss: 1.2233 Validation Accuracy: 0.538400\n",
      "Epoch 17, CIFAR-10 Batch 3:  0.555\n",
      "Loss: 1.0657 Validation Accuracy: 0.555000\n",
      "Epoch 17, CIFAR-10 Batch 4:  0.5508\n",
      "Loss: 1.1127 Validation Accuracy: 0.550800\n",
      "Epoch 17, CIFAR-10 Batch 5:  0.5424\n",
      "Loss: 1.2058 Validation Accuracy: 0.542400\n",
      "Epoch 18, CIFAR-10 Batch 1:  0.5484\n",
      "Loss: 1.2506 Validation Accuracy: 0.548400\n",
      "Epoch 18, CIFAR-10 Batch 2:  0.546\n",
      "Loss: 1.2130 Validation Accuracy: 0.546000\n",
      "Epoch 18, CIFAR-10 Batch 3:  0.5566\n",
      "Loss: 1.0540 Validation Accuracy: 0.556600\n",
      "Epoch 18, CIFAR-10 Batch 4:  0.5556\n",
      "Loss: 1.1197 Validation Accuracy: 0.555600\n",
      "Epoch 18, CIFAR-10 Batch 5:  0.5488\n",
      "Loss: 1.1842 Validation Accuracy: 0.548800\n",
      "Epoch 19, CIFAR-10 Batch 1:  0.5434\n",
      "Loss: 1.2287 Validation Accuracy: 0.543400\n",
      "Epoch 19, CIFAR-10 Batch 2:  0.556\n",
      "Loss: 1.1841 Validation Accuracy: 0.556000\n",
      "Epoch 19, CIFAR-10 Batch 3:  0.5554\n",
      "Loss: 1.0444 Validation Accuracy: 0.555400\n",
      "Epoch 19, CIFAR-10 Batch 4:  0.5548\n",
      "Loss: 1.1063 Validation Accuracy: 0.554800\n",
      "Epoch 19, CIFAR-10 Batch 5:  0.5566\n",
      "Loss: 1.1670 Validation Accuracy: 0.556600\n",
      "Epoch 20, CIFAR-10 Batch 1:  0.55\n",
      "Loss: 1.2211 Validation Accuracy: 0.550000\n",
      "Epoch 20, CIFAR-10 Batch 2:  0.5546\n",
      "Loss: 1.1739 Validation Accuracy: 0.554600\n",
      "Epoch 20, CIFAR-10 Batch 3:  0.5626\n",
      "Loss: 1.0441 Validation Accuracy: 0.562600\n",
      "Epoch 20, CIFAR-10 Batch 4:  0.5574\n",
      "Loss: 1.0948 Validation Accuracy: 0.557400\n",
      "Epoch 20, CIFAR-10 Batch 5:  0.5544\n",
      "Loss: 1.1756 Validation Accuracy: 0.554400\n",
      "Epoch 21, CIFAR-10 Batch 1:  0.557\n",
      "Loss: 1.2087 Validation Accuracy: 0.557000\n",
      "Epoch 21, CIFAR-10 Batch 2:  0.5586\n",
      "Loss: 1.1670 Validation Accuracy: 0.558600\n",
      "Epoch 21, CIFAR-10 Batch 3:  0.5684\n",
      "Loss: 1.0498 Validation Accuracy: 0.568400\n",
      "Epoch 21, CIFAR-10 Batch 4:  0.5672\n",
      "Loss: 1.0808 Validation Accuracy: 0.567200\n",
      "Epoch 21, CIFAR-10 Batch 5:  0.5654\n",
      "Loss: 1.1423 Validation Accuracy: 0.565400\n",
      "Epoch 22, CIFAR-10 Batch 1:  0.5532\n",
      "Loss: 1.1992 Validation Accuracy: 0.553200\n",
      "Epoch 22, CIFAR-10 Batch 2:  0.5554\n",
      "Loss: 1.1545 Validation Accuracy: 0.555400\n",
      "Epoch 22, CIFAR-10 Batch 3:  0.5728\n",
      "Loss: 1.0103 Validation Accuracy: 0.572800\n",
      "Epoch 22, CIFAR-10 Batch 4:  0.5702\n",
      "Loss: 1.0586 Validation Accuracy: 0.570200\n",
      "Epoch 22, CIFAR-10 Batch 5:  0.5598\n",
      "Loss: 1.1454 Validation Accuracy: 0.559800\n",
      "Epoch 23, CIFAR-10 Batch 1:  0.564\n",
      "Loss: 1.1699 Validation Accuracy: 0.564000\n",
      "Epoch 23, CIFAR-10 Batch 2:  0.5726\n",
      "Loss: 1.1363 Validation Accuracy: 0.572600\n",
      "Epoch 23, CIFAR-10 Batch 3:  0.5618\n",
      "Loss: 1.0015 Validation Accuracy: 0.561800\n",
      "Epoch 23, CIFAR-10 Batch 4:  0.5732\n",
      "Loss: 1.0532 Validation Accuracy: 0.573200\n",
      "Epoch 23, CIFAR-10 Batch 5:  0.5676\n",
      "Loss: 1.1153 Validation Accuracy: 0.567600\n",
      "Epoch 24, CIFAR-10 Batch 1:  0.5566\n",
      "Loss: 1.1639 Validation Accuracy: 0.556600\n",
      "Epoch 24, CIFAR-10 Batch 2:  0.5696\n",
      "Loss: 1.1214 Validation Accuracy: 0.569600\n",
      "Epoch 24, CIFAR-10 Batch 3:  0.575\n",
      "Loss: 0.9994 Validation Accuracy: 0.575000\n",
      "Epoch 24, CIFAR-10 Batch 4:  0.5604\n",
      "Loss: 1.0755 Validation Accuracy: 0.560400\n",
      "Epoch 24, CIFAR-10 Batch 5:  0.575\n",
      "Loss: 1.0860 Validation Accuracy: 0.575000\n",
      "Epoch 25, CIFAR-10 Batch 1:  0.5684\n",
      "Loss: 1.1505 Validation Accuracy: 0.568400\n",
      "Epoch 25, CIFAR-10 Batch 2:  0.566\n",
      "Loss: 1.1337 Validation Accuracy: 0.566000\n",
      "Epoch 25, CIFAR-10 Batch 3:  0.567\n",
      "Loss: 1.0084 Validation Accuracy: 0.567000\n",
      "Epoch 25, CIFAR-10 Batch 4:  0.576\n",
      "Loss: 1.0407 Validation Accuracy: 0.576000\n",
      "Epoch 25, CIFAR-10 Batch 5:  0.5746\n",
      "Loss: 1.0785 Validation Accuracy: 0.574600\n",
      "Epoch 26, CIFAR-10 Batch 1:  0.5588\n",
      "Loss: 1.1504 Validation Accuracy: 0.558800\n",
      "Epoch 26, CIFAR-10 Batch 2:  0.582\n",
      "Loss: 1.0998 Validation Accuracy: 0.582000\n",
      "Epoch 26, CIFAR-10 Batch 3:  0.5874\n",
      "Loss: 0.9591 Validation Accuracy: 0.587400\n",
      "Epoch 26, CIFAR-10 Batch 4:  0.578\n",
      "Loss: 1.0193 Validation Accuracy: 0.578000\n",
      "Epoch 26, CIFAR-10 Batch 5:  0.5774\n",
      "Loss: 1.0579 Validation Accuracy: 0.577400\n",
      "Epoch 27, CIFAR-10 Batch 1:  0.5652\n",
      "Loss: 1.1387 Validation Accuracy: 0.565200\n",
      "Epoch 27, CIFAR-10 Batch 2:  0.5728\n",
      "Loss: 1.0999 Validation Accuracy: 0.572800\n",
      "Epoch 27, CIFAR-10 Batch 3:  0.5838\n",
      "Loss: 0.9555 Validation Accuracy: 0.583800\n",
      "Epoch 27, CIFAR-10 Batch 4:  0.5934\n",
      "Loss: 1.0055 Validation Accuracy: 0.593400\n",
      "Epoch 27, CIFAR-10 Batch 5:  0.5718\n",
      "Loss: 1.0672 Validation Accuracy: 0.571800\n",
      "Epoch 28, CIFAR-10 Batch 1:  0.5778\n",
      "Loss: 1.1192 Validation Accuracy: 0.577800\n",
      "Epoch 28, CIFAR-10 Batch 2:  0.576\n",
      "Loss: 1.0865 Validation Accuracy: 0.576000\n",
      "Epoch 28, CIFAR-10 Batch 3:  0.5798\n",
      "Loss: 0.9733 Validation Accuracy: 0.579800\n",
      "Epoch 28, CIFAR-10 Batch 4:  0.59\n",
      "Loss: 0.9978 Validation Accuracy: 0.590000\n",
      "Epoch 28, CIFAR-10 Batch 5:  0.586\n",
      "Loss: 1.0627 Validation Accuracy: 0.586000\n",
      "Epoch 29, CIFAR-10 Batch 1:  0.5722\n",
      "Loss: 1.1218 Validation Accuracy: 0.572200\n",
      "Epoch 29, CIFAR-10 Batch 2:  0.5732\n",
      "Loss: 1.0863 Validation Accuracy: 0.573200\n",
      "Epoch 29, CIFAR-10 Batch 3:  0.5792\n",
      "Loss: 0.9500 Validation Accuracy: 0.579200\n",
      "Epoch 29, CIFAR-10 Batch 4:  0.5884\n",
      "Loss: 0.9868 Validation Accuracy: 0.588400\n",
      "Epoch 29, CIFAR-10 Batch 5:  0.5866\n",
      "Loss: 1.0566 Validation Accuracy: 0.586600\n",
      "Epoch 30, CIFAR-10 Batch 1:  0.5886\n",
      "Loss: 1.0985 Validation Accuracy: 0.588600\n",
      "Epoch 30, CIFAR-10 Batch 2:  0.5808\n",
      "Loss: 1.0800 Validation Accuracy: 0.580800\n",
      "Epoch 30, CIFAR-10 Batch 3:  0.5954\n",
      "Loss: 0.9486 Validation Accuracy: 0.595400\n",
      "Epoch 30, CIFAR-10 Batch 4:  0.5872\n",
      "Loss: 0.9891 Validation Accuracy: 0.587200\n",
      "Epoch 30, CIFAR-10 Batch 5:  0.5834\n",
      "Loss: 1.0445 Validation Accuracy: 0.583400\n",
      "Epoch 31, CIFAR-10 Batch 1:  0.5818\n",
      "Loss: 1.0981 Validation Accuracy: 0.581800\n",
      "Epoch 31, CIFAR-10 Batch 2:  0.5912\n",
      "Loss: 1.0557 Validation Accuracy: 0.591200\n",
      "Epoch 31, CIFAR-10 Batch 3:  0.59\n",
      "Loss: 0.9398 Validation Accuracy: 0.590000\n",
      "Epoch 31, CIFAR-10 Batch 4:  0.5898\n",
      "Loss: 0.9834 Validation Accuracy: 0.589800\n",
      "Epoch 31, CIFAR-10 Batch 5:  0.5936\n",
      "Loss: 1.0261 Validation Accuracy: 0.593600\n",
      "Epoch 32, CIFAR-10 Batch 1:  0.5828\n",
      "Loss: 1.0843 Validation Accuracy: 0.582800\n",
      "Epoch 32, CIFAR-10 Batch 2:  0.586\n",
      "Loss: 1.0488 Validation Accuracy: 0.586000\n",
      "Epoch 32, CIFAR-10 Batch 3:  0.589\n",
      "Loss: 0.9333 Validation Accuracy: 0.589000\n",
      "Epoch 32, CIFAR-10 Batch 4:  0.5972\n",
      "Loss: 0.9662 Validation Accuracy: 0.597200\n",
      "Epoch 32, CIFAR-10 Batch 5:  0.59\n",
      "Loss: 1.0351 Validation Accuracy: 0.590000\n",
      "Epoch 33, CIFAR-10 Batch 1:  0.587\n",
      "Loss: 1.0723 Validation Accuracy: 0.587000\n",
      "Epoch 33, CIFAR-10 Batch 2:  0.5858\n",
      "Loss: 1.0554 Validation Accuracy: 0.585800\n",
      "Epoch 33, CIFAR-10 Batch 3:  0.5942\n",
      "Loss: 0.9053 Validation Accuracy: 0.594200\n",
      "Epoch 33, CIFAR-10 Batch 4:  0.6018\n",
      "Loss: 0.9672 Validation Accuracy: 0.601800\n",
      "Epoch 33, CIFAR-10 Batch 5:  0.5996\n",
      "Loss: 1.0159 Validation Accuracy: 0.599600\n",
      "Epoch 34, CIFAR-10 Batch 1:  0.5914\n",
      "Loss: 1.0749 Validation Accuracy: 0.591400\n",
      "Epoch 34, CIFAR-10 Batch 2:  0.5956\n",
      "Loss: 1.0477 Validation Accuracy: 0.595600\n",
      "Epoch 34, CIFAR-10 Batch 3:  0.5942\n",
      "Loss: 0.9202 Validation Accuracy: 0.594200\n",
      "Epoch 34, CIFAR-10 Batch 4:  0.5922\n",
      "Loss: 0.9589 Validation Accuracy: 0.592200\n",
      "Epoch 34, CIFAR-10 Batch 5:  0.599\n",
      "Loss: 1.0124 Validation Accuracy: 0.599000\n",
      "Epoch 35, CIFAR-10 Batch 1:  0.5978\n",
      "Loss: 1.0536 Validation Accuracy: 0.597800\n",
      "Epoch 35, CIFAR-10 Batch 2:  0.6044\n",
      "Loss: 1.0126 Validation Accuracy: 0.604400\n",
      "Epoch 35, CIFAR-10 Batch 3:  0.6014\n",
      "Loss: 0.9042 Validation Accuracy: 0.601400\n",
      "Epoch 35, CIFAR-10 Batch 4:  0.6068\n",
      "Loss: 0.9474 Validation Accuracy: 0.606800\n",
      "Epoch 35, CIFAR-10 Batch 5:  0.5968\n",
      "Loss: 0.9873 Validation Accuracy: 0.596800\n",
      "Epoch 36, CIFAR-10 Batch 1:  0.592\n",
      "Loss: 1.0486 Validation Accuracy: 0.592000\n",
      "Epoch 36, CIFAR-10 Batch 2:  0.6\n",
      "Loss: 1.0222 Validation Accuracy: 0.600000\n",
      "Epoch 36, CIFAR-10 Batch 3:  0.6064\n",
      "Loss: 0.9164 Validation Accuracy: 0.606400\n",
      "Epoch 36, CIFAR-10 Batch 4:  0.591\n",
      "Loss: 0.9522 Validation Accuracy: 0.591000\n",
      "Epoch 36, CIFAR-10 Batch 5:  0.6046\n",
      "Loss: 0.9884 Validation Accuracy: 0.604600\n",
      "Epoch 37, CIFAR-10 Batch 1:  0.598\n",
      "Loss: 1.0346 Validation Accuracy: 0.598000\n",
      "Epoch 37, CIFAR-10 Batch 2:  0.5998\n",
      "Loss: 1.0083 Validation Accuracy: 0.599800\n",
      "Epoch 37, CIFAR-10 Batch 3:  0.6006\n",
      "Loss: 0.8826 Validation Accuracy: 0.600600\n",
      "Epoch 37, CIFAR-10 Batch 4:  0.6004\n",
      "Loss: 0.9484 Validation Accuracy: 0.600400\n",
      "Epoch 37, CIFAR-10 Batch 5:  0.5936\n",
      "Loss: 0.9727 Validation Accuracy: 0.593600\n",
      "Epoch 38, CIFAR-10 Batch 1:  0.5954\n",
      "Loss: 1.0319 Validation Accuracy: 0.595400\n",
      "Epoch 38, CIFAR-10 Batch 2:  0.592\n",
      "Loss: 1.0266 Validation Accuracy: 0.592000\n",
      "Epoch 38, CIFAR-10 Batch 3:  0.5972\n",
      "Loss: 0.8909 Validation Accuracy: 0.597200\n",
      "Epoch 38, CIFAR-10 Batch 4:  0.6042\n",
      "Loss: 0.9319 Validation Accuracy: 0.604200\n",
      "Epoch 38, CIFAR-10 Batch 5:  0.6078\n",
      "Loss: 0.9698 Validation Accuracy: 0.607800\n",
      "Epoch 39, CIFAR-10 Batch 1:  0.5998\n",
      "Loss: 1.0264 Validation Accuracy: 0.599800\n",
      "Epoch 39, CIFAR-10 Batch 2:  0.597\n",
      "Loss: 1.0261 Validation Accuracy: 0.597000\n",
      "Epoch 39, CIFAR-10 Batch 3:  0.5988\n",
      "Loss: 0.8963 Validation Accuracy: 0.598800\n",
      "Epoch 39, CIFAR-10 Batch 4:  0.6046\n",
      "Loss: 0.9306 Validation Accuracy: 0.604600\n",
      "Epoch 39, CIFAR-10 Batch 5:  0.6052\n",
      "Loss: 0.9916 Validation Accuracy: 0.605200\n",
      "Epoch 40, CIFAR-10 Batch 1:  0.5984\n",
      "Loss: 1.0337 Validation Accuracy: 0.598400\n",
      "Epoch 40, CIFAR-10 Batch 2:  0.6072\n",
      "Loss: 0.9847 Validation Accuracy: 0.607200\n",
      "Epoch 40, CIFAR-10 Batch 3:  0.608\n",
      "Loss: 0.8842 Validation Accuracy: 0.608000\n",
      "Epoch 40, CIFAR-10 Batch 4:  0.6058\n",
      "Loss: 0.9182 Validation Accuracy: 0.605800\n",
      "Epoch 40, CIFAR-10 Batch 5:  0.611\n",
      "Loss: 0.9662 Validation Accuracy: 0.611000\n",
      "Epoch 41, CIFAR-10 Batch 1:  0.5928\n",
      "Loss: 1.0341 Validation Accuracy: 0.592800\n",
      "Epoch 41, CIFAR-10 Batch 2:  0.5872\n",
      "Loss: 0.9982 Validation Accuracy: 0.587200\n",
      "Epoch 41, CIFAR-10 Batch 3:  0.5952\n",
      "Loss: 0.8838 Validation Accuracy: 0.595200\n",
      "Epoch 41, CIFAR-10 Batch 4:  0.602\n",
      "Loss: 0.9083 Validation Accuracy: 0.602000\n",
      "Epoch 41, CIFAR-10 Batch 5:  0.6112\n",
      "Loss: 0.9520 Validation Accuracy: 0.611200\n",
      "Epoch 42, CIFAR-10 Batch 1:  0.601\n",
      "Loss: 0.9964 Validation Accuracy: 0.601000\n",
      "Epoch 42, CIFAR-10 Batch 2:  0.6016\n",
      "Loss: 0.9828 Validation Accuracy: 0.601600\n",
      "Epoch 42, CIFAR-10 Batch 3:  0.6078\n",
      "Loss: 0.8507 Validation Accuracy: 0.607800\n",
      "Epoch 42, CIFAR-10 Batch 4:  0.606\n",
      "Loss: 0.9136 Validation Accuracy: 0.606000\n",
      "Epoch 42, CIFAR-10 Batch 5:  0.594\n",
      "Loss: 0.9689 Validation Accuracy: 0.594000\n",
      "Epoch 43, CIFAR-10 Batch 1:  0.603\n",
      "Loss: 1.0296 Validation Accuracy: 0.603000\n",
      "Epoch 43, CIFAR-10 Batch 2:  0.6114\n",
      "Loss: 0.9521 Validation Accuracy: 0.611400\n",
      "Epoch 43, CIFAR-10 Batch 3:  0.6108\n",
      "Loss: 0.8769 Validation Accuracy: 0.610800\n",
      "Epoch 43, CIFAR-10 Batch 4:  0.609\n",
      "Loss: 0.9000 Validation Accuracy: 0.609000\n",
      "Epoch 43, CIFAR-10 Batch 5:  0.6074\n",
      "Loss: 0.9425 Validation Accuracy: 0.607400\n",
      "Epoch 44, CIFAR-10 Batch 1:  0.6014\n",
      "Loss: 1.0012 Validation Accuracy: 0.601400\n",
      "Epoch 44, CIFAR-10 Batch 2:  0.6098\n",
      "Loss: 0.9636 Validation Accuracy: 0.609800\n",
      "Epoch 44, CIFAR-10 Batch 3:  0.6068\n",
      "Loss: 0.8648 Validation Accuracy: 0.606800\n",
      "Epoch 44, CIFAR-10 Batch 4:  0.6076\n",
      "Loss: 0.9112 Validation Accuracy: 0.607600\n",
      "Epoch 44, CIFAR-10 Batch 5:  0.6042\n",
      "Loss: 0.9519 Validation Accuracy: 0.604200\n",
      "Epoch 45, CIFAR-10 Batch 1:  0.6052\n",
      "Loss: 1.0128 Validation Accuracy: 0.605200\n",
      "Epoch 45, CIFAR-10 Batch 2:  0.6096\n",
      "Loss: 0.9706 Validation Accuracy: 0.609600\n",
      "Epoch 45, CIFAR-10 Batch 3:  0.603\n",
      "Loss: 0.8505 Validation Accuracy: 0.603000\n",
      "Epoch 45, CIFAR-10 Batch 4:  0.615\n",
      "Loss: 0.8922 Validation Accuracy: 0.615000\n",
      "Epoch 45, CIFAR-10 Batch 5:  0.6114\n",
      "Loss: 0.9329 Validation Accuracy: 0.611400\n",
      "Epoch 46, CIFAR-10 Batch 1:  0.6104\n",
      "Loss: 0.9768 Validation Accuracy: 0.610400\n",
      "Epoch 46, CIFAR-10 Batch 2:  0.6106\n",
      "Loss: 0.9537 Validation Accuracy: 0.610600\n",
      "Epoch 46, CIFAR-10 Batch 3:  0.6186\n",
      "Loss: 0.8328 Validation Accuracy: 0.618600\n",
      "Epoch 46, CIFAR-10 Batch 4:  0.6122\n",
      "Loss: 0.8925 Validation Accuracy: 0.612200\n",
      "Epoch 46, CIFAR-10 Batch 5:  0.6154\n",
      "Loss: 0.9214 Validation Accuracy: 0.615400\n",
      "Epoch 47, CIFAR-10 Batch 1:  0.6146\n",
      "Loss: 0.9754 Validation Accuracy: 0.614600\n",
      "Epoch 47, CIFAR-10 Batch 2:  0.6124\n",
      "Loss: 0.9495 Validation Accuracy: 0.612400\n",
      "Epoch 47, CIFAR-10 Batch 3:  0.6078\n",
      "Loss: 0.8400 Validation Accuracy: 0.607800\n",
      "Epoch 47, CIFAR-10 Batch 4:  0.6142\n",
      "Loss: 0.8758 Validation Accuracy: 0.614200\n",
      "Epoch 47, CIFAR-10 Batch 5:  0.6108\n",
      "Loss: 0.9171 Validation Accuracy: 0.610800\n",
      "Epoch 48, CIFAR-10 Batch 1:  0.6118\n",
      "Loss: 0.9678 Validation Accuracy: 0.611800\n",
      "Epoch 48, CIFAR-10 Batch 2:  0.6196\n",
      "Loss: 0.9259 Validation Accuracy: 0.619600\n",
      "Epoch 48, CIFAR-10 Batch 3:  0.6034\n",
      "Loss: 0.8297 Validation Accuracy: 0.603400\n",
      "Epoch 48, CIFAR-10 Batch 4:  0.6242\n",
      "Loss: 0.8645 Validation Accuracy: 0.624200\n",
      "Epoch 48, CIFAR-10 Batch 5:  0.6104\n",
      "Loss: 0.9081 Validation Accuracy: 0.610400\n",
      "Epoch 49, CIFAR-10 Batch 1:  0.6212\n",
      "Loss: 0.9594 Validation Accuracy: 0.621200\n",
      "Epoch 49, CIFAR-10 Batch 2:  0.62\n",
      "Loss: 0.9283 Validation Accuracy: 0.620000\n",
      "Epoch 49, CIFAR-10 Batch 3:  0.6072\n",
      "Loss: 0.8366 Validation Accuracy: 0.607200\n",
      "Epoch 49, CIFAR-10 Batch 4:  0.6184\n",
      "Loss: 0.8803 Validation Accuracy: 0.618400\n",
      "Epoch 49, CIFAR-10 Batch 5:  0.611\n",
      "Loss: 0.9106 Validation Accuracy: 0.611000\n",
      "Epoch 50, CIFAR-10 Batch 1:  0.6112\n",
      "Loss: 0.9654 Validation Accuracy: 0.611200\n",
      "Epoch 50, CIFAR-10 Batch 2:  0.6062\n",
      "Loss: 0.9418 Validation Accuracy: 0.606200\n",
      "Epoch 50, CIFAR-10 Batch 3:  0.6232\n",
      "Loss: 0.8182 Validation Accuracy: 0.623200\n",
      "Epoch 50, CIFAR-10 Batch 4:  0.6148\n",
      "Loss: 0.8620 Validation Accuracy: 0.614800\n",
      "Epoch 50, CIFAR-10 Batch 5:  0.6078\n",
      "Loss: 0.9202 Validation Accuracy: 0.607800\n",
      "Epoch 51, CIFAR-10 Batch 1:  0.6218\n",
      "Loss: 0.9419 Validation Accuracy: 0.621800\n",
      "Epoch 51, CIFAR-10 Batch 2:  0.6186\n",
      "Loss: 0.9140 Validation Accuracy: 0.618600\n",
      "Epoch 51, CIFAR-10 Batch 3:  0.613\n",
      "Loss: 0.8191 Validation Accuracy: 0.613000\n",
      "Epoch 51, CIFAR-10 Batch 4:  0.6072\n",
      "Loss: 0.8777 Validation Accuracy: 0.607200\n",
      "Epoch 51, CIFAR-10 Batch 5:  0.5988\n",
      "Loss: 0.9353 Validation Accuracy: 0.598800\n",
      "Epoch 52, CIFAR-10 Batch 1:  0.6054\n",
      "Loss: 0.9680 Validation Accuracy: 0.605400\n",
      "Epoch 52, CIFAR-10 Batch 2:  0.6014\n",
      "Loss: 0.9343 Validation Accuracy: 0.601400\n",
      "Epoch 52, CIFAR-10 Batch 3:  0.5902\n",
      "Loss: 0.8495 Validation Accuracy: 0.590200\n",
      "Epoch 52, CIFAR-10 Batch 4:  0.6128\n",
      "Loss: 0.8874 Validation Accuracy: 0.612800\n",
      "Epoch 52, CIFAR-10 Batch 5:  0.6134\n",
      "Loss: 0.8814 Validation Accuracy: 0.613400\n",
      "Epoch 53, CIFAR-10 Batch 1:  0.6112\n",
      "Loss: 0.9284 Validation Accuracy: 0.611200\n",
      "Epoch 53, CIFAR-10 Batch 2:  0.6068\n",
      "Loss: 0.9089 Validation Accuracy: 0.606800\n",
      "Epoch 53, CIFAR-10 Batch 3:  0.6182\n",
      "Loss: 0.7999 Validation Accuracy: 0.618200\n",
      "Epoch 53, CIFAR-10 Batch 4:  0.613\n",
      "Loss: 0.8344 Validation Accuracy: 0.613000\n",
      "Epoch 53, CIFAR-10 Batch 5:  0.6044\n",
      "Loss: 0.8961 Validation Accuracy: 0.604400\n",
      "Epoch 54, CIFAR-10 Batch 1:  0.6208\n",
      "Loss: 0.9428 Validation Accuracy: 0.620800\n",
      "Epoch 54, CIFAR-10 Batch 2:  0.622\n",
      "Loss: 0.8923 Validation Accuracy: 0.622000\n",
      "Epoch 54, CIFAR-10 Batch 3:  0.6082\n",
      "Loss: 0.8177 Validation Accuracy: 0.608200\n",
      "Epoch 54, CIFAR-10 Batch 4:  0.6162\n",
      "Loss: 0.8530 Validation Accuracy: 0.616200\n",
      "Epoch 54, CIFAR-10 Batch 5:  0.6018\n",
      "Loss: 0.8953 Validation Accuracy: 0.601800\n",
      "Epoch 55, CIFAR-10 Batch 1:  0.6118\n",
      "Loss: 0.9436 Validation Accuracy: 0.611800\n",
      "Epoch 55, CIFAR-10 Batch 2:  0.6194\n",
      "Loss: 0.8970 Validation Accuracy: 0.619400\n",
      "Epoch 55, CIFAR-10 Batch 3:  0.6188\n",
      "Loss: 0.8122 Validation Accuracy: 0.618800\n",
      "Epoch 55, CIFAR-10 Batch 4:  0.6164\n",
      "Loss: 0.8531 Validation Accuracy: 0.616400\n",
      "Epoch 55, CIFAR-10 Batch 5:  0.6014\n",
      "Loss: 0.8878 Validation Accuracy: 0.601400\n",
      "Epoch 56, CIFAR-10 Batch 1:  0.6202\n",
      "Loss: 0.9049 Validation Accuracy: 0.620200\n",
      "Epoch 56, CIFAR-10 Batch 2:  0.62\n",
      "Loss: 0.8887 Validation Accuracy: 0.620000\n",
      "Epoch 56, CIFAR-10 Batch 3:  0.617\n",
      "Loss: 0.7977 Validation Accuracy: 0.617000\n",
      "Epoch 56, CIFAR-10 Batch 4:  0.6176\n",
      "Loss: 0.8386 Validation Accuracy: 0.617600\n",
      "Epoch 56, CIFAR-10 Batch 5:  0.6086\n",
      "Loss: 0.8891 Validation Accuracy: 0.608600\n",
      "Epoch 57, CIFAR-10 Batch 1:  0.6232\n",
      "Loss: 0.8989 Validation Accuracy: 0.623200\n",
      "Epoch 57, CIFAR-10 Batch 2:  0.6194\n",
      "Loss: 0.8704 Validation Accuracy: 0.619400\n",
      "Epoch 57, CIFAR-10 Batch 3:  0.62\n",
      "Loss: 0.8037 Validation Accuracy: 0.620000\n",
      "Epoch 57, CIFAR-10 Batch 4:  0.6206\n",
      "Loss: 0.8490 Validation Accuracy: 0.620600\n",
      "Epoch 57, CIFAR-10 Batch 5:  0.6114\n",
      "Loss: 0.8729 Validation Accuracy: 0.611400\n",
      "Epoch 58, CIFAR-10 Batch 1:  0.6214\n",
      "Loss: 0.9246 Validation Accuracy: 0.621400\n",
      "Epoch 58, CIFAR-10 Batch 2:  0.6064\n",
      "Loss: 0.8909 Validation Accuracy: 0.606400\n",
      "Epoch 58, CIFAR-10 Batch 3:  0.6156\n",
      "Loss: 0.7965 Validation Accuracy: 0.615600\n",
      "Epoch 58, CIFAR-10 Batch 4:  0.6228\n",
      "Loss: 0.8280 Validation Accuracy: 0.622800\n",
      "Epoch 58, CIFAR-10 Batch 5:  0.6148\n",
      "Loss: 0.8831 Validation Accuracy: 0.614800\n",
      "Epoch 59, CIFAR-10 Batch 1:  0.6258\n",
      "Loss: 0.9085 Validation Accuracy: 0.625800\n",
      "Epoch 59, CIFAR-10 Batch 2:  0.6264\n",
      "Loss: 0.8754 Validation Accuracy: 0.626400\n",
      "Epoch 59, CIFAR-10 Batch 3:  0.6204\n",
      "Loss: 0.7925 Validation Accuracy: 0.620400\n",
      "Epoch 59, CIFAR-10 Batch 4:  0.6226\n",
      "Loss: 0.8290 Validation Accuracy: 0.622600\n",
      "Epoch 59, CIFAR-10 Batch 5:  0.6222\n",
      "Loss: 0.8455 Validation Accuracy: 0.622200\n",
      "Epoch 60, CIFAR-10 Batch 1:  0.6222\n",
      "Loss: 0.9094 Validation Accuracy: 0.622200\n",
      "Epoch 60, CIFAR-10 Batch 2:  0.6204\n",
      "Loss: 0.8597 Validation Accuracy: 0.620400\n",
      "Epoch 60, CIFAR-10 Batch 3:  0.6092\n",
      "Loss: 0.8218 Validation Accuracy: 0.609200\n",
      "Epoch 60, CIFAR-10 Batch 4:  0.628\n",
      "Loss: 0.8168 Validation Accuracy: 0.628000\n",
      "Epoch 60, CIFAR-10 Batch 5:  0.6212\n",
      "Loss: 0.8356 Validation Accuracy: 0.621200\n",
      "Epoch 61, CIFAR-10 Batch 1:  0.6262\n",
      "Loss: 0.8797 Validation Accuracy: 0.626200\n",
      "Epoch 61, CIFAR-10 Batch 2:  0.6232\n",
      "Loss: 0.8711 Validation Accuracy: 0.623200\n",
      "Epoch 61, CIFAR-10 Batch 3:  0.6288\n",
      "Loss: 0.7565 Validation Accuracy: 0.628800\n",
      "Epoch 61, CIFAR-10 Batch 4:  0.6258\n",
      "Loss: 0.8117 Validation Accuracy: 0.625800\n",
      "Epoch 61, CIFAR-10 Batch 5:  0.6188\n",
      "Loss: 0.8361 Validation Accuracy: 0.618800\n",
      "Epoch 62, CIFAR-10 Batch 1:  0.6162\n",
      "Loss: 0.9060 Validation Accuracy: 0.616200\n",
      "Epoch 62, CIFAR-10 Batch 2:  0.6116\n",
      "Loss: 0.8591 Validation Accuracy: 0.611600\n",
      "Epoch 62, CIFAR-10 Batch 3:  0.6318\n",
      "Loss: 0.7522 Validation Accuracy: 0.631800\n",
      "Epoch 62, CIFAR-10 Batch 4:  0.6136\n",
      "Loss: 0.8273 Validation Accuracy: 0.613600\n",
      "Epoch 62, CIFAR-10 Batch 5:  0.6176\n",
      "Loss: 0.8461 Validation Accuracy: 0.617600\n",
      "Epoch 63, CIFAR-10 Batch 1:  0.6282\n",
      "Loss: 0.9026 Validation Accuracy: 0.628200\n",
      "Epoch 63, CIFAR-10 Batch 2:  0.6136\n",
      "Loss: 0.8622 Validation Accuracy: 0.613600\n",
      "Epoch 63, CIFAR-10 Batch 3:  0.6256\n",
      "Loss: 0.7652 Validation Accuracy: 0.625600\n",
      "Epoch 63, CIFAR-10 Batch 4:  0.6282\n",
      "Loss: 0.8125 Validation Accuracy: 0.628200\n",
      "Epoch 63, CIFAR-10 Batch 5:  0.6258\n",
      "Loss: 0.8280 Validation Accuracy: 0.625800\n",
      "Epoch 64, CIFAR-10 Batch 1:  0.617\n",
      "Loss: 0.8779 Validation Accuracy: 0.617000\n",
      "Epoch 64, CIFAR-10 Batch 2:  0.629\n",
      "Loss: 0.8514 Validation Accuracy: 0.629000\n",
      "Epoch 64, CIFAR-10 Batch 3:  0.6194\n",
      "Loss: 0.7489 Validation Accuracy: 0.619400\n",
      "Epoch 64, CIFAR-10 Batch 4:  0.6246\n",
      "Loss: 0.8065 Validation Accuracy: 0.624600\n",
      "Epoch 64, CIFAR-10 Batch 5:  0.6236\n",
      "Loss: 0.8264 Validation Accuracy: 0.623600\n",
      "Epoch 65, CIFAR-10 Batch 1:  0.6172\n",
      "Loss: 0.9014 Validation Accuracy: 0.617200\n",
      "Epoch 65, CIFAR-10 Batch 2:  0.626\n",
      "Loss: 0.8430 Validation Accuracy: 0.626000\n",
      "Epoch 65, CIFAR-10 Batch 3:  0.6208\n",
      "Loss: 0.7577 Validation Accuracy: 0.620800\n",
      "Epoch 65, CIFAR-10 Batch 4:  0.6244\n",
      "Loss: 0.8509 Validation Accuracy: 0.624400\n",
      "Epoch 65, CIFAR-10 Batch 5:  0.6202\n",
      "Loss: 0.8358 Validation Accuracy: 0.620200\n",
      "Epoch 66, CIFAR-10 Batch 1:  0.6216\n",
      "Loss: 0.8767 Validation Accuracy: 0.621600\n",
      "Epoch 66, CIFAR-10 Batch 2:  0.6374\n",
      "Loss: 0.8347 Validation Accuracy: 0.637400\n",
      "Epoch 66, CIFAR-10 Batch 3:  0.6278\n",
      "Loss: 0.7463 Validation Accuracy: 0.627800\n",
      "Epoch 66, CIFAR-10 Batch 4:  0.6282\n",
      "Loss: 0.7910 Validation Accuracy: 0.628200\n",
      "Epoch 66, CIFAR-10 Batch 5:  0.62\n",
      "Loss: 0.8304 Validation Accuracy: 0.620000\n",
      "Epoch 67, CIFAR-10 Batch 1:  0.6186\n",
      "Loss: 0.8742 Validation Accuracy: 0.618600\n",
      "Epoch 67, CIFAR-10 Batch 2:  0.6284\n",
      "Loss: 0.8402 Validation Accuracy: 0.628400\n",
      "Epoch 67, CIFAR-10 Batch 3:  0.629\n",
      "Loss: 0.7452 Validation Accuracy: 0.629000\n",
      "Epoch 67, CIFAR-10 Batch 4:  0.6288\n",
      "Loss: 0.7860 Validation Accuracy: 0.628800\n",
      "Epoch 67, CIFAR-10 Batch 5:  0.6326\n",
      "Loss: 0.8062 Validation Accuracy: 0.632600\n",
      "Epoch 68, CIFAR-10 Batch 1:  0.6284\n",
      "Loss: 0.8618 Validation Accuracy: 0.628400\n",
      "Epoch 68, CIFAR-10 Batch 2:  0.6286\n",
      "Loss: 0.8269 Validation Accuracy: 0.628600\n",
      "Epoch 68, CIFAR-10 Batch 3:  0.6178\n",
      "Loss: 0.7576 Validation Accuracy: 0.617800\n",
      "Epoch 68, CIFAR-10 Batch 4:  0.6306\n",
      "Loss: 0.7830 Validation Accuracy: 0.630600\n",
      "Epoch 68, CIFAR-10 Batch 5:  0.6224\n",
      "Loss: 0.8112 Validation Accuracy: 0.622400\n",
      "Epoch 69, CIFAR-10 Batch 1:  0.6202\n",
      "Loss: 0.8700 Validation Accuracy: 0.620200\n",
      "Epoch 69, CIFAR-10 Batch 2:  0.6302\n",
      "Loss: 0.8212 Validation Accuracy: 0.630200\n",
      "Epoch 69, CIFAR-10 Batch 3:  0.6246\n",
      "Loss: 0.7582 Validation Accuracy: 0.624600\n",
      "Epoch 69, CIFAR-10 Batch 4:  0.6304\n",
      "Loss: 0.7869 Validation Accuracy: 0.630400\n",
      "Epoch 69, CIFAR-10 Batch 5:  0.6176\n",
      "Loss: 0.8161 Validation Accuracy: 0.617600\n",
      "Epoch 70, CIFAR-10 Batch 1:  0.6342\n",
      "Loss: 0.8586 Validation Accuracy: 0.634200\n",
      "Epoch 70, CIFAR-10 Batch 2:  0.632\n",
      "Loss: 0.8216 Validation Accuracy: 0.632000\n",
      "Epoch 70, CIFAR-10 Batch 3:  0.629\n",
      "Loss: 0.7450 Validation Accuracy: 0.629000\n",
      "Epoch 70, CIFAR-10 Batch 4:  0.631\n",
      "Loss: 0.7833 Validation Accuracy: 0.631000\n",
      "Epoch 70, CIFAR-10 Batch 5:  0.6234\n",
      "Loss: 0.7978 Validation Accuracy: 0.623400\n",
      "Epoch 71, CIFAR-10 Batch 1:  0.636\n",
      "Loss: 0.8494 Validation Accuracy: 0.636000\n",
      "Epoch 71, CIFAR-10 Batch 2:  0.6284\n",
      "Loss: 0.8027 Validation Accuracy: 0.628400\n",
      "Epoch 71, CIFAR-10 Batch 3:  0.623\n",
      "Loss: 0.7304 Validation Accuracy: 0.623000\n",
      "Epoch 71, CIFAR-10 Batch 4:  0.6292\n",
      "Loss: 0.7712 Validation Accuracy: 0.629200\n",
      "Epoch 71, CIFAR-10 Batch 5:  0.6286\n",
      "Loss: 0.8006 Validation Accuracy: 0.628600\n",
      "Epoch 72, CIFAR-10 Batch 1:  0.623\n",
      "Loss: 0.8603 Validation Accuracy: 0.623000\n",
      "Epoch 72, CIFAR-10 Batch 2:  0.6302\n",
      "Loss: 0.8049 Validation Accuracy: 0.630200\n",
      "Epoch 72, CIFAR-10 Batch 3:  0.6274\n",
      "Loss: 0.7534 Validation Accuracy: 0.627400\n",
      "Epoch 72, CIFAR-10 Batch 4:  0.6274\n",
      "Loss: 0.7818 Validation Accuracy: 0.627400\n",
      "Epoch 72, CIFAR-10 Batch 5:  0.6282\n",
      "Loss: 0.8017 Validation Accuracy: 0.628200\n",
      "Epoch 73, CIFAR-10 Batch 1:  0.6274\n",
      "Loss: 0.8694 Validation Accuracy: 0.627400\n",
      "Epoch 73, CIFAR-10 Batch 2:  0.6292\n",
      "Loss: 0.8120 Validation Accuracy: 0.629200\n",
      "Epoch 73, CIFAR-10 Batch 3:  0.6284\n",
      "Loss: 0.7293 Validation Accuracy: 0.628400\n",
      "Epoch 73, CIFAR-10 Batch 4:  0.6344\n",
      "Loss: 0.7565 Validation Accuracy: 0.634400\n",
      "Epoch 73, CIFAR-10 Batch 5:  0.628\n",
      "Loss: 0.7758 Validation Accuracy: 0.628000\n",
      "Epoch 74, CIFAR-10 Batch 1:  0.63\n",
      "Loss: 0.8379 Validation Accuracy: 0.630000\n",
      "Epoch 74, CIFAR-10 Batch 2:  0.6322\n",
      "Loss: 0.8218 Validation Accuracy: 0.632200\n",
      "Epoch 74, CIFAR-10 Batch 3:  0.6342\n",
      "Loss: 0.7269 Validation Accuracy: 0.634200\n",
      "Epoch 74, CIFAR-10 Batch 4:  0.6248\n",
      "Loss: 0.7551 Validation Accuracy: 0.624800\n",
      "Epoch 74, CIFAR-10 Batch 5:  0.621\n",
      "Loss: 0.7937 Validation Accuracy: 0.621000\n",
      "Epoch 75, CIFAR-10 Batch 1:  0.6314\n",
      "Loss: 0.8398 Validation Accuracy: 0.631400\n",
      "Epoch 75, CIFAR-10 Batch 2:  0.6282\n",
      "Loss: 0.8108 Validation Accuracy: 0.628200\n",
      "Epoch 75, CIFAR-10 Batch 3:  0.625\n",
      "Loss: 0.7404 Validation Accuracy: 0.625000\n",
      "Epoch 75, CIFAR-10 Batch 4:  0.6338\n",
      "Loss: 0.7736 Validation Accuracy: 0.633800\n",
      "Epoch 75, CIFAR-10 Batch 5:  0.6352\n",
      "Loss: 0.7604 Validation Accuracy: 0.635200\n",
      "Epoch 76, CIFAR-10 Batch 1:  0.6384\n",
      "Loss: 0.8296 Validation Accuracy: 0.638400\n",
      "Epoch 76, CIFAR-10 Batch 2:  0.6344\n",
      "Loss: 0.7888 Validation Accuracy: 0.634400\n",
      "Epoch 76, CIFAR-10 Batch 3:  0.635\n",
      "Loss: 0.7171 Validation Accuracy: 0.635000\n",
      "Epoch 76, CIFAR-10 Batch 4:  0.638\n",
      "Loss: 0.7522 Validation Accuracy: 0.638000\n",
      "Epoch 76, CIFAR-10 Batch 5:  0.6274\n",
      "Loss: 0.7694 Validation Accuracy: 0.627400\n",
      "Epoch 77, CIFAR-10 Batch 1:  0.6348\n",
      "Loss: 0.8370 Validation Accuracy: 0.634800\n",
      "Epoch 77, CIFAR-10 Batch 2:  0.6378\n",
      "Loss: 0.8006 Validation Accuracy: 0.637800\n",
      "Epoch 77, CIFAR-10 Batch 3:  0.6382\n",
      "Loss: 0.6952 Validation Accuracy: 0.638200\n",
      "Epoch 77, CIFAR-10 Batch 4:  0.6324\n",
      "Loss: 0.7498 Validation Accuracy: 0.632400\n",
      "Epoch 77, CIFAR-10 Batch 5:  0.6218\n",
      "Loss: 0.7800 Validation Accuracy: 0.621800\n",
      "Epoch 78, CIFAR-10 Batch 1:  0.626\n",
      "Loss: 0.8243 Validation Accuracy: 0.626000\n",
      "Epoch 78, CIFAR-10 Batch 2:  0.6266\n",
      "Loss: 0.7975 Validation Accuracy: 0.626600\n",
      "Epoch 78, CIFAR-10 Batch 3:  0.6348\n",
      "Loss: 0.7061 Validation Accuracy: 0.634800\n",
      "Epoch 78, CIFAR-10 Batch 4:  0.6334\n",
      "Loss: 0.7484 Validation Accuracy: 0.633400\n",
      "Epoch 78, CIFAR-10 Batch 5:  0.6342\n",
      "Loss: 0.7726 Validation Accuracy: 0.634200\n",
      "Epoch 79, CIFAR-10 Batch 1:  0.6288\n",
      "Loss: 0.8406 Validation Accuracy: 0.628800\n",
      "Epoch 79, CIFAR-10 Batch 2:  0.6214\n",
      "Loss: 0.7931 Validation Accuracy: 0.621400\n",
      "Epoch 79, CIFAR-10 Batch 3:  0.6316\n",
      "Loss: 0.7108 Validation Accuracy: 0.631600\n",
      "Epoch 79, CIFAR-10 Batch 4:  0.6326\n",
      "Loss: 0.7448 Validation Accuracy: 0.632600\n",
      "Epoch 79, CIFAR-10 Batch 5:  0.6326\n",
      "Loss: 0.7654 Validation Accuracy: 0.632600\n",
      "Epoch 80, CIFAR-10 Batch 1:  0.6342\n",
      "Loss: 0.8279 Validation Accuracy: 0.634200\n",
      "Epoch 80, CIFAR-10 Batch 2:  0.6328\n",
      "Loss: 0.7949 Validation Accuracy: 0.632800\n",
      "Epoch 80, CIFAR-10 Batch 3:  0.6362\n",
      "Loss: 0.7152 Validation Accuracy: 0.636200\n",
      "Epoch 80, CIFAR-10 Batch 4:  0.6342\n",
      "Loss: 0.7540 Validation Accuracy: 0.634200\n",
      "Epoch 80, CIFAR-10 Batch 5:  0.6198\n",
      "Loss: 0.7857 Validation Accuracy: 0.619800\n",
      "Epoch 81, CIFAR-10 Batch 1:  0.625\n",
      "Loss: 0.8273 Validation Accuracy: 0.625000\n",
      "Epoch 81, CIFAR-10 Batch 2:  0.6418\n",
      "Loss: 0.7646 Validation Accuracy: 0.641800\n",
      "Epoch 81, CIFAR-10 Batch 3:  0.6292\n",
      "Loss: 0.7050 Validation Accuracy: 0.629200\n",
      "Epoch 81, CIFAR-10 Batch 4:  0.6374\n",
      "Loss: 0.7391 Validation Accuracy: 0.637400\n",
      "Epoch 81, CIFAR-10 Batch 5:  0.6102\n",
      "Loss: 0.8201 Validation Accuracy: 0.610200\n",
      "Epoch 82, CIFAR-10 Batch 1:  0.6392\n",
      "Loss: 0.8123 Validation Accuracy: 0.639200\n",
      "Epoch 82, CIFAR-10 Batch 2:  0.6342\n",
      "Loss: 0.7916 Validation Accuracy: 0.634200\n",
      "Epoch 82, CIFAR-10 Batch 3:  0.634\n",
      "Loss: 0.7148 Validation Accuracy: 0.634000\n",
      "Epoch 82, CIFAR-10 Batch 4:  0.63\n",
      "Loss: 0.7532 Validation Accuracy: 0.630000\n",
      "Epoch 82, CIFAR-10 Batch 5:  0.6124\n",
      "Loss: 0.7757 Validation Accuracy: 0.612400\n",
      "Epoch 83, CIFAR-10 Batch 1:  0.6366\n",
      "Loss: 0.8112 Validation Accuracy: 0.636600\n",
      "Epoch 83, CIFAR-10 Batch 2:  0.6288\n",
      "Loss: 0.7771 Validation Accuracy: 0.628800\n",
      "Epoch 83, CIFAR-10 Batch 3:  0.6248\n",
      "Loss: 0.7112 Validation Accuracy: 0.624800\n",
      "Epoch 83, CIFAR-10 Batch 4:  0.623\n",
      "Loss: 0.7414 Validation Accuracy: 0.623000\n",
      "Epoch 83, CIFAR-10 Batch 5:  0.6398\n",
      "Loss: 0.7716 Validation Accuracy: 0.639800\n",
      "Epoch 84, CIFAR-10 Batch 1:  0.6358\n",
      "Loss: 0.8122 Validation Accuracy: 0.635800\n",
      "Epoch 84, CIFAR-10 Batch 2:  0.6304\n",
      "Loss: 0.7675 Validation Accuracy: 0.630400\n",
      "Epoch 84, CIFAR-10 Batch 3:  0.6244\n",
      "Loss: 0.7032 Validation Accuracy: 0.624400\n",
      "Epoch 84, CIFAR-10 Batch 4:  0.6254\n",
      "Loss: 0.7384 Validation Accuracy: 0.625400\n",
      "Epoch 84, CIFAR-10 Batch 5:  0.6288\n",
      "Loss: 0.7596 Validation Accuracy: 0.628800\n",
      "Epoch 85, CIFAR-10 Batch 1:  0.6326\n",
      "Loss: 0.7968 Validation Accuracy: 0.632600\n",
      "Epoch 85, CIFAR-10 Batch 2:  0.6356\n",
      "Loss: 0.7627 Validation Accuracy: 0.635600\n",
      "Epoch 85, CIFAR-10 Batch 3:  0.617\n",
      "Loss: 0.7153 Validation Accuracy: 0.617000\n",
      "Epoch 85, CIFAR-10 Batch 4:  0.6346\n",
      "Loss: 0.7443 Validation Accuracy: 0.634600\n",
      "Epoch 85, CIFAR-10 Batch 5:  0.637\n",
      "Loss: 0.7676 Validation Accuracy: 0.637000\n",
      "Epoch 86, CIFAR-10 Batch 1:  0.6332\n",
      "Loss: 0.8139 Validation Accuracy: 0.633200\n",
      "Epoch 86, CIFAR-10 Batch 2:  0.639\n",
      "Loss: 0.7767 Validation Accuracy: 0.639000\n",
      "Epoch 86, CIFAR-10 Batch 3:  0.6394\n",
      "Loss: 0.6827 Validation Accuracy: 0.639400\n",
      "Epoch 86, CIFAR-10 Batch 4:  0.628\n",
      "Loss: 0.7511 Validation Accuracy: 0.628000\n",
      "Epoch 86, CIFAR-10 Batch 5:  0.6304\n",
      "Loss: 0.7710 Validation Accuracy: 0.630400\n",
      "Epoch 87, CIFAR-10 Batch 1:  0.6392\n",
      "Loss: 0.7936 Validation Accuracy: 0.639200\n",
      "Epoch 87, CIFAR-10 Batch 2:  0.6348\n",
      "Loss: 0.7704 Validation Accuracy: 0.634800\n",
      "Epoch 87, CIFAR-10 Batch 3:  0.6258\n",
      "Loss: 0.6957 Validation Accuracy: 0.625800\n",
      "Epoch 87, CIFAR-10 Batch 4:  0.6292\n",
      "Loss: 0.7562 Validation Accuracy: 0.629200\n",
      "Epoch 87, CIFAR-10 Batch 5:  0.6264\n",
      "Loss: 0.7525 Validation Accuracy: 0.626400\n",
      "Epoch 88, CIFAR-10 Batch 1:  0.6368\n",
      "Loss: 0.7801 Validation Accuracy: 0.636800\n",
      "Epoch 88, CIFAR-10 Batch 2:  0.6388\n",
      "Loss: 0.7645 Validation Accuracy: 0.638800\n",
      "Epoch 88, CIFAR-10 Batch 3:  0.6388\n",
      "Loss: 0.6665 Validation Accuracy: 0.638800\n",
      "Epoch 88, CIFAR-10 Batch 4:  0.6306\n",
      "Loss: 0.7297 Validation Accuracy: 0.630600\n",
      "Epoch 88, CIFAR-10 Batch 5:  0.6376\n",
      "Loss: 0.7351 Validation Accuracy: 0.637600\n",
      "Epoch 89, CIFAR-10 Batch 1:  0.6392\n",
      "Loss: 0.7892 Validation Accuracy: 0.639200\n",
      "Epoch 89, CIFAR-10 Batch 2:  0.6384\n",
      "Loss: 0.7614 Validation Accuracy: 0.638400\n",
      "Epoch 89, CIFAR-10 Batch 3:  0.6332\n",
      "Loss: 0.6761 Validation Accuracy: 0.633200\n",
      "Epoch 89, CIFAR-10 Batch 4:  0.64\n",
      "Loss: 0.7221 Validation Accuracy: 0.640000\n",
      "Epoch 89, CIFAR-10 Batch 5:  0.6356\n",
      "Loss: 0.7311 Validation Accuracy: 0.635600\n",
      "Epoch 90, CIFAR-10 Batch 1:  0.636\n",
      "Loss: 0.7874 Validation Accuracy: 0.636000\n",
      "Epoch 90, CIFAR-10 Batch 2:  0.6312\n",
      "Loss: 0.7480 Validation Accuracy: 0.631200\n",
      "Epoch 90, CIFAR-10 Batch 3:  0.6426\n",
      "Loss: 0.6712 Validation Accuracy: 0.642600\n",
      "Epoch 90, CIFAR-10 Batch 4:  0.645\n",
      "Loss: 0.7133 Validation Accuracy: 0.645000\n",
      "Epoch 90, CIFAR-10 Batch 5:  0.6442\n",
      "Loss: 0.7229 Validation Accuracy: 0.644200\n",
      "Epoch 91, CIFAR-10 Batch 1:  0.632\n",
      "Loss: 0.8101 Validation Accuracy: 0.632000\n",
      "Epoch 91, CIFAR-10 Batch 2:  0.6398\n",
      "Loss: 0.7582 Validation Accuracy: 0.639800\n",
      "Epoch 91, CIFAR-10 Batch 3:  0.6416\n",
      "Loss: 0.6817 Validation Accuracy: 0.641600\n",
      "Epoch 91, CIFAR-10 Batch 4:  0.6328\n",
      "Loss: 0.7220 Validation Accuracy: 0.632800\n",
      "Epoch 91, CIFAR-10 Batch 5:  0.6464\n",
      "Loss: 0.7223 Validation Accuracy: 0.646400\n",
      "Epoch 92, CIFAR-10 Batch 1:  0.6412\n",
      "Loss: 0.7767 Validation Accuracy: 0.641200\n",
      "Epoch 92, CIFAR-10 Batch 2:  0.629\n",
      "Loss: 0.7447 Validation Accuracy: 0.629000\n",
      "Epoch 92, CIFAR-10 Batch 3:  0.6466\n",
      "Loss: 0.6675 Validation Accuracy: 0.646600\n",
      "Epoch 92, CIFAR-10 Batch 4:  0.636\n",
      "Loss: 0.7001 Validation Accuracy: 0.636000\n",
      "Epoch 92, CIFAR-10 Batch 5:  0.6324\n",
      "Loss: 0.7322 Validation Accuracy: 0.632400\n",
      "Epoch 93, CIFAR-10 Batch 1:  0.6306\n",
      "Loss: 0.7983 Validation Accuracy: 0.630600\n",
      "Epoch 93, CIFAR-10 Batch 2:  0.6358\n",
      "Loss: 0.7461 Validation Accuracy: 0.635800\n",
      "Epoch 93, CIFAR-10 Batch 3:  0.6398\n",
      "Loss: 0.6717 Validation Accuracy: 0.639800\n",
      "Epoch 93, CIFAR-10 Batch 4:  0.6426\n",
      "Loss: 0.7148 Validation Accuracy: 0.642600\n",
      "Epoch 93, CIFAR-10 Batch 5:  0.6314\n",
      "Loss: 0.7348 Validation Accuracy: 0.631400\n",
      "Epoch 94, CIFAR-10 Batch 1:  0.6452\n",
      "Loss: 0.7648 Validation Accuracy: 0.645200\n",
      "Epoch 94, CIFAR-10 Batch 2:  0.6384\n",
      "Loss: 0.7438 Validation Accuracy: 0.638400\n",
      "Epoch 94, CIFAR-10 Batch 3:  0.6404\n",
      "Loss: 0.6674 Validation Accuracy: 0.640400\n",
      "Epoch 94, CIFAR-10 Batch 4:  0.6368\n",
      "Loss: 0.7156 Validation Accuracy: 0.636800\n",
      "Epoch 94, CIFAR-10 Batch 5:  0.6378\n",
      "Loss: 0.7333 Validation Accuracy: 0.637800\n",
      "Epoch 95, CIFAR-10 Batch 1:  0.6366\n",
      "Loss: 0.7714 Validation Accuracy: 0.636600\n",
      "Epoch 95, CIFAR-10 Batch 2:  0.6362\n",
      "Loss: 0.7620 Validation Accuracy: 0.636200\n",
      "Epoch 95, CIFAR-10 Batch 3:  0.6372\n",
      "Loss: 0.6642 Validation Accuracy: 0.637200\n",
      "Epoch 95, CIFAR-10 Batch 4:  0.63\n",
      "Loss: 0.7255 Validation Accuracy: 0.630000\n",
      "Epoch 95, CIFAR-10 Batch 5:  0.639\n",
      "Loss: 0.7360 Validation Accuracy: 0.639000\n",
      "Epoch 96, CIFAR-10 Batch 1:  0.6456\n",
      "Loss: 0.7476 Validation Accuracy: 0.645600\n",
      "Epoch 96, CIFAR-10 Batch 2:  0.6452\n",
      "Loss: 0.7351 Validation Accuracy: 0.645200\n",
      "Epoch 96, CIFAR-10 Batch 3:  0.6336\n",
      "Loss: 0.6592 Validation Accuracy: 0.633600\n",
      "Epoch 96, CIFAR-10 Batch 4:  0.6318\n",
      "Loss: 0.7103 Validation Accuracy: 0.631800\n",
      "Epoch 96, CIFAR-10 Batch 5:  0.632\n",
      "Loss: 0.7470 Validation Accuracy: 0.632000\n",
      "Epoch 97, CIFAR-10 Batch 1:  0.631\n",
      "Loss: 0.7575 Validation Accuracy: 0.631000\n",
      "Epoch 97, CIFAR-10 Batch 2:  0.6348\n",
      "Loss: 0.7427 Validation Accuracy: 0.634800\n",
      "Epoch 97, CIFAR-10 Batch 3:  0.643\n",
      "Loss: 0.6565 Validation Accuracy: 0.643000\n",
      "Epoch 97, CIFAR-10 Batch 4:  0.6364\n",
      "Loss: 0.7071 Validation Accuracy: 0.636400\n",
      "Epoch 97, CIFAR-10 Batch 5:  0.6342\n",
      "Loss: 0.7258 Validation Accuracy: 0.634200\n",
      "Epoch 98, CIFAR-10 Batch 1:  0.644\n",
      "Loss: 0.7819 Validation Accuracy: 0.644000\n",
      "Epoch 98, CIFAR-10 Batch 2:  0.6246\n",
      "Loss: 0.7534 Validation Accuracy: 0.624600\n",
      "Epoch 98, CIFAR-10 Batch 3:  0.6284\n",
      "Loss: 0.6811 Validation Accuracy: 0.628400\n",
      "Epoch 98, CIFAR-10 Batch 4:  0.6372\n",
      "Loss: 0.6972 Validation Accuracy: 0.637200\n",
      "Epoch 98, CIFAR-10 Batch 5:  0.626\n",
      "Loss: 0.7184 Validation Accuracy: 0.626000\n",
      "Epoch 99, CIFAR-10 Batch 1:  0.6458\n",
      "Loss: 0.7512 Validation Accuracy: 0.645800\n",
      "Epoch 99, CIFAR-10 Batch 2:  0.6266\n",
      "Loss: 0.7440 Validation Accuracy: 0.626600\n",
      "Epoch 99, CIFAR-10 Batch 3:  0.6376\n",
      "Loss: 0.6723 Validation Accuracy: 0.637600\n",
      "Epoch 99, CIFAR-10 Batch 4:  0.6384\n",
      "Loss: 0.6932 Validation Accuracy: 0.638400\n",
      "Epoch 99, CIFAR-10 Batch 5:  0.6328\n",
      "Loss: 0.7162 Validation Accuracy: 0.632800\n",
      "Epoch 100, CIFAR-10 Batch 1:  0.6354\n",
      "Loss: 0.7647 Validation Accuracy: 0.635400\n",
      "Epoch 100, CIFAR-10 Batch 2:  0.6276\n",
      "Loss: 0.7297 Validation Accuracy: 0.627600\n",
      "Epoch 100, CIFAR-10 Batch 3:  0.634\n",
      "Loss: 0.6697 Validation Accuracy: 0.634000\n",
      "Epoch 100, CIFAR-10 Batch 4:  0.6402\n",
      "Loss: 0.7001 Validation Accuracy: 0.640200\n",
      "Epoch 100, CIFAR-10 Batch 5:  0.6358\n",
      "Loss: 0.7108 Validation Accuracy: 0.635800\n",
      "Epoch 101, CIFAR-10 Batch 1:  0.6456\n",
      "Loss: 0.7432 Validation Accuracy: 0.645600\n",
      "Epoch 101, CIFAR-10 Batch 2:  0.6432\n",
      "Loss: 0.7191 Validation Accuracy: 0.643200\n",
      "Epoch 101, CIFAR-10 Batch 3:  0.6388\n",
      "Loss: 0.6584 Validation Accuracy: 0.638800\n",
      "Epoch 101, CIFAR-10 Batch 4:  0.639\n",
      "Loss: 0.7070 Validation Accuracy: 0.639000\n",
      "Epoch 101, CIFAR-10 Batch 5:  0.6372\n",
      "Loss: 0.7176 Validation Accuracy: 0.637200\n",
      "Epoch 102, CIFAR-10 Batch 1:  0.6444\n",
      "Loss: 0.7628 Validation Accuracy: 0.644400\n",
      "Epoch 102, CIFAR-10 Batch 2:  0.6406\n",
      "Loss: 0.7153 Validation Accuracy: 0.640600\n",
      "Epoch 102, CIFAR-10 Batch 3:  0.6352\n",
      "Loss: 0.6509 Validation Accuracy: 0.635200\n",
      "Epoch 102, CIFAR-10 Batch 4:  0.635\n",
      "Loss: 0.6917 Validation Accuracy: 0.635000\n",
      "Epoch 102, CIFAR-10 Batch 5:  0.6408\n",
      "Loss: 0.6977 Validation Accuracy: 0.640800\n",
      "Epoch 103, CIFAR-10 Batch 1:  0.6404\n",
      "Loss: 0.7367 Validation Accuracy: 0.640400\n",
      "Epoch 103, CIFAR-10 Batch 2:  0.647\n",
      "Loss: 0.7082 Validation Accuracy: 0.647000\n",
      "Epoch 103, CIFAR-10 Batch 3:  0.639\n",
      "Loss: 0.6439 Validation Accuracy: 0.639000\n",
      "Epoch 103, CIFAR-10 Batch 4:  0.6308\n",
      "Loss: 0.6989 Validation Accuracy: 0.630800\n",
      "Epoch 103, CIFAR-10 Batch 5:  0.6348\n",
      "Loss: 0.6968 Validation Accuracy: 0.634800\n",
      "Epoch 104, CIFAR-10 Batch 1:  0.633\n",
      "Loss: 0.7510 Validation Accuracy: 0.633000\n",
      "Epoch 104, CIFAR-10 Batch 2:  0.6502\n",
      "Loss: 0.7278 Validation Accuracy: 0.650200\n",
      "Epoch 104, CIFAR-10 Batch 3:  0.6422\n",
      "Loss: 0.6577 Validation Accuracy: 0.642200\n",
      "Epoch 104, CIFAR-10 Batch 4:  0.6388\n",
      "Loss: 0.7065 Validation Accuracy: 0.638800\n",
      "Epoch 104, CIFAR-10 Batch 5:  0.6298\n",
      "Loss: 0.7124 Validation Accuracy: 0.629800\n",
      "Epoch 105, CIFAR-10 Batch 1:  0.646\n",
      "Loss: 0.7583 Validation Accuracy: 0.646000\n",
      "Epoch 105, CIFAR-10 Batch 2:  0.6424\n",
      "Loss: 0.7280 Validation Accuracy: 0.642400\n",
      "Epoch 105, CIFAR-10 Batch 3:  0.6408\n",
      "Loss: 0.6335 Validation Accuracy: 0.640800\n",
      "Epoch 105, CIFAR-10 Batch 4:  0.635\n",
      "Loss: 0.6965 Validation Accuracy: 0.635000\n",
      "Epoch 105, CIFAR-10 Batch 5:  0.6358\n",
      "Loss: 0.7029 Validation Accuracy: 0.635800\n",
      "Epoch 106, CIFAR-10 Batch 1:  0.6442\n",
      "Loss: 0.7545 Validation Accuracy: 0.644200\n",
      "Epoch 106, CIFAR-10 Batch 2:  0.644\n",
      "Loss: 0.7113 Validation Accuracy: 0.644000\n",
      "Epoch 106, CIFAR-10 Batch 3:  0.6442\n",
      "Loss: 0.6458 Validation Accuracy: 0.644200\n",
      "Epoch 106, CIFAR-10 Batch 4:  0.6448\n",
      "Loss: 0.6680 Validation Accuracy: 0.644800\n",
      "Epoch 106, CIFAR-10 Batch 5:  0.6362\n",
      "Loss: 0.6970 Validation Accuracy: 0.636200\n",
      "Epoch 107, CIFAR-10 Batch 1:  0.6424\n",
      "Loss: 0.7348 Validation Accuracy: 0.642400\n",
      "Epoch 107, CIFAR-10 Batch 2:  0.6324\n",
      "Loss: 0.7126 Validation Accuracy: 0.632400\n",
      "Epoch 107, CIFAR-10 Batch 3:  0.6392\n",
      "Loss: 0.6419 Validation Accuracy: 0.639200\n",
      "Epoch 107, CIFAR-10 Batch 4:  0.6394\n",
      "Loss: 0.6822 Validation Accuracy: 0.639400\n",
      "Epoch 107, CIFAR-10 Batch 5:  0.6448\n",
      "Loss: 0.6920 Validation Accuracy: 0.644800\n",
      "Epoch 108, CIFAR-10 Batch 1:  0.646\n",
      "Loss: 0.7324 Validation Accuracy: 0.646000\n",
      "Epoch 108, CIFAR-10 Batch 2:  0.643\n",
      "Loss: 0.7105 Validation Accuracy: 0.643000\n",
      "Epoch 108, CIFAR-10 Batch 3:  0.6388\n",
      "Loss: 0.6405 Validation Accuracy: 0.638800\n",
      "Epoch 108, CIFAR-10 Batch 4:  0.635\n",
      "Loss: 0.6788 Validation Accuracy: 0.635000\n",
      "Epoch 108, CIFAR-10 Batch 5:  0.6396\n",
      "Loss: 0.6962 Validation Accuracy: 0.639600\n",
      "Epoch 109, CIFAR-10 Batch 1:  0.645\n",
      "Loss: 0.7456 Validation Accuracy: 0.645000\n",
      "Epoch 109, CIFAR-10 Batch 2:  0.6498\n",
      "Loss: 0.7181 Validation Accuracy: 0.649800\n",
      "Epoch 109, CIFAR-10 Batch 3:  0.638\n",
      "Loss: 0.6292 Validation Accuracy: 0.638000\n",
      "Epoch 109, CIFAR-10 Batch 4:  0.6338\n",
      "Loss: 0.6740 Validation Accuracy: 0.633800\n",
      "Epoch 109, CIFAR-10 Batch 5:  0.643\n",
      "Loss: 0.6924 Validation Accuracy: 0.643000\n",
      "Epoch 110, CIFAR-10 Batch 1:  0.648\n",
      "Loss: 0.7340 Validation Accuracy: 0.648000\n",
      "Epoch 110, CIFAR-10 Batch 2:  0.6468\n",
      "Loss: 0.7072 Validation Accuracy: 0.646800\n",
      "Epoch 110, CIFAR-10 Batch 3:  0.6326\n",
      "Loss: 0.6430 Validation Accuracy: 0.632600\n",
      "Epoch 110, CIFAR-10 Batch 4:  0.6396\n",
      "Loss: 0.7115 Validation Accuracy: 0.639600\n",
      "Epoch 110, CIFAR-10 Batch 5:  0.6508\n",
      "Loss: 0.6817 Validation Accuracy: 0.650800\n",
      "Epoch 111, CIFAR-10 Batch 1:  0.6468\n",
      "Loss: 0.7385 Validation Accuracy: 0.646800\n",
      "Epoch 111, CIFAR-10 Batch 2:  0.6458\n",
      "Loss: 0.7012 Validation Accuracy: 0.645800\n",
      "Epoch 111, CIFAR-10 Batch 3:  0.6428\n",
      "Loss: 0.6401 Validation Accuracy: 0.642800\n",
      "Epoch 111, CIFAR-10 Batch 4:  0.643\n",
      "Loss: 0.6848 Validation Accuracy: 0.643000\n",
      "Epoch 111, CIFAR-10 Batch 5:  0.6506\n",
      "Loss: 0.6971 Validation Accuracy: 0.650600\n",
      "Epoch 112, CIFAR-10 Batch 1:  0.6484\n",
      "Loss: 0.7309 Validation Accuracy: 0.648400\n",
      "Epoch 112, CIFAR-10 Batch 2:  0.6492\n",
      "Loss: 0.7042 Validation Accuracy: 0.649200\n",
      "Epoch 112, CIFAR-10 Batch 3:  0.6452\n",
      "Loss: 0.6531 Validation Accuracy: 0.645200\n",
      "Epoch 112, CIFAR-10 Batch 4:  0.642\n",
      "Loss: 0.6744 Validation Accuracy: 0.642000\n",
      "Epoch 112, CIFAR-10 Batch 5:  0.6414\n",
      "Loss: 0.6805 Validation Accuracy: 0.641400\n",
      "Epoch 113, CIFAR-10 Batch 1:  0.6492\n",
      "Loss: 0.7228 Validation Accuracy: 0.649200\n",
      "Epoch 113, CIFAR-10 Batch 2:  0.6444\n",
      "Loss: 0.6928 Validation Accuracy: 0.644400\n",
      "Epoch 113, CIFAR-10 Batch 3:  0.6486\n",
      "Loss: 0.6204 Validation Accuracy: 0.648600\n",
      "Epoch 113, CIFAR-10 Batch 4:  0.6344\n",
      "Loss: 0.6777 Validation Accuracy: 0.634400\n",
      "Epoch 113, CIFAR-10 Batch 5:  0.6394\n",
      "Loss: 0.6723 Validation Accuracy: 0.639400\n",
      "Epoch 114, CIFAR-10 Batch 1:  0.6308\n",
      "Loss: 0.7568 Validation Accuracy: 0.630800\n",
      "Epoch 114, CIFAR-10 Batch 2:  0.6264\n",
      "Loss: 0.6944 Validation Accuracy: 0.626400\n",
      "Epoch 114, CIFAR-10 Batch 3:  0.6468\n",
      "Loss: 0.6333 Validation Accuracy: 0.646800\n",
      "Epoch 114, CIFAR-10 Batch 4:  0.6436\n",
      "Loss: 0.6888 Validation Accuracy: 0.643600\n",
      "Epoch 114, CIFAR-10 Batch 5:  0.6422\n",
      "Loss: 0.6861 Validation Accuracy: 0.642200\n",
      "Epoch 115, CIFAR-10 Batch 1:  0.6422\n",
      "Loss: 0.7106 Validation Accuracy: 0.642200\n",
      "Epoch 115, CIFAR-10 Batch 2:  0.637\n",
      "Loss: 0.7062 Validation Accuracy: 0.637000\n",
      "Epoch 115, CIFAR-10 Batch 3:  0.6426\n",
      "Loss: 0.6291 Validation Accuracy: 0.642600\n",
      "Epoch 115, CIFAR-10 Batch 4:  0.6428\n",
      "Loss: 0.6573 Validation Accuracy: 0.642800\n",
      "Epoch 115, CIFAR-10 Batch 5:  0.642\n",
      "Loss: 0.6842 Validation Accuracy: 0.642000\n",
      "Epoch 116, CIFAR-10 Batch 1:  0.6494\n",
      "Loss: 0.7097 Validation Accuracy: 0.649400\n",
      "Epoch 116, CIFAR-10 Batch 2:  0.6402\n",
      "Loss: 0.6998 Validation Accuracy: 0.640200\n",
      "Epoch 116, CIFAR-10 Batch 3:  0.6474\n",
      "Loss: 0.6246 Validation Accuracy: 0.647400\n",
      "Epoch 116, CIFAR-10 Batch 4:  0.647\n",
      "Loss: 0.6420 Validation Accuracy: 0.647000\n",
      "Epoch 116, CIFAR-10 Batch 5:  0.6508\n",
      "Loss: 0.6757 Validation Accuracy: 0.650800\n",
      "Epoch 117, CIFAR-10 Batch 1:  0.6456\n",
      "Loss: 0.7275 Validation Accuracy: 0.645600\n",
      "Epoch 117, CIFAR-10 Batch 2:  0.647\n",
      "Loss: 0.6938 Validation Accuracy: 0.647000\n",
      "Epoch 117, CIFAR-10 Batch 3:  0.6446\n",
      "Loss: 0.6201 Validation Accuracy: 0.644600\n",
      "Epoch 117, CIFAR-10 Batch 4:  0.6448\n",
      "Loss: 0.6744 Validation Accuracy: 0.644800\n",
      "Epoch 117, CIFAR-10 Batch 5:  0.6338\n",
      "Loss: 0.6949 Validation Accuracy: 0.633800\n",
      "Epoch 118, CIFAR-10 Batch 1:  0.6406\n",
      "Loss: 0.7161 Validation Accuracy: 0.640600\n",
      "Epoch 118, CIFAR-10 Batch 2:  0.6486\n",
      "Loss: 0.6816 Validation Accuracy: 0.648600\n",
      "Epoch 118, CIFAR-10 Batch 3:  0.646\n",
      "Loss: 0.6393 Validation Accuracy: 0.646000\n",
      "Epoch 118, CIFAR-10 Batch 4:  0.6398\n",
      "Loss: 0.6455 Validation Accuracy: 0.639800\n",
      "Epoch 118, CIFAR-10 Batch 5:  0.6454\n",
      "Loss: 0.6694 Validation Accuracy: 0.645400\n",
      "Epoch 119, CIFAR-10 Batch 1:  0.6426\n",
      "Loss: 0.7099 Validation Accuracy: 0.642600\n",
      "Epoch 119, CIFAR-10 Batch 2:  0.6312\n",
      "Loss: 0.6931 Validation Accuracy: 0.631200\n",
      "Epoch 119, CIFAR-10 Batch 3:  0.642\n",
      "Loss: 0.6377 Validation Accuracy: 0.642000\n",
      "Epoch 119, CIFAR-10 Batch 4:  0.6284\n",
      "Loss: 0.6706 Validation Accuracy: 0.628400\n",
      "Epoch 119, CIFAR-10 Batch 5:  0.6464\n",
      "Loss: 0.6679 Validation Accuracy: 0.646400\n",
      "Epoch 120, CIFAR-10 Batch 1:  0.639\n",
      "Loss: 0.7145 Validation Accuracy: 0.639000\n",
      "Epoch 120, CIFAR-10 Batch 2:  0.6296\n",
      "Loss: 0.6998 Validation Accuracy: 0.629600\n",
      "Epoch 120, CIFAR-10 Batch 3:  0.6432\n",
      "Loss: 0.6329 Validation Accuracy: 0.643200\n",
      "Epoch 120, CIFAR-10 Batch 4:  0.6364\n",
      "Loss: 0.6714 Validation Accuracy: 0.636400\n",
      "Epoch 120, CIFAR-10 Batch 5:  0.6436\n",
      "Loss: 0.6584 Validation Accuracy: 0.643600\n",
      "Epoch 121, CIFAR-10 Batch 1:  0.6322\n",
      "Loss: 0.7435 Validation Accuracy: 0.632200\n",
      "Epoch 121, CIFAR-10 Batch 2:  0.6294\n",
      "Loss: 0.6991 Validation Accuracy: 0.629400\n",
      "Epoch 121, CIFAR-10 Batch 3:  0.6424\n",
      "Loss: 0.6308 Validation Accuracy: 0.642400\n",
      "Epoch 121, CIFAR-10 Batch 4:  0.6418\n",
      "Loss: 0.6561 Validation Accuracy: 0.641800\n",
      "Epoch 121, CIFAR-10 Batch 5:  0.645\n",
      "Loss: 0.6674 Validation Accuracy: 0.645000\n",
      "Epoch 122, CIFAR-10 Batch 1:  0.6422\n",
      "Loss: 0.7156 Validation Accuracy: 0.642200\n",
      "Epoch 122, CIFAR-10 Batch 2:  0.6452\n",
      "Loss: 0.6883 Validation Accuracy: 0.645200\n",
      "Epoch 122, CIFAR-10 Batch 3:  0.6408\n",
      "Loss: 0.6228 Validation Accuracy: 0.640800\n",
      "Epoch 122, CIFAR-10 Batch 4:  0.6358\n",
      "Loss: 0.6569 Validation Accuracy: 0.635800\n",
      "Epoch 122, CIFAR-10 Batch 5:  0.6444\n",
      "Loss: 0.6595 Validation Accuracy: 0.644400\n",
      "Epoch 123, CIFAR-10 Batch 1:  0.6336\n",
      "Loss: 0.7152 Validation Accuracy: 0.633600\n",
      "Epoch 123, CIFAR-10 Batch 2:  0.6352\n",
      "Loss: 0.6662 Validation Accuracy: 0.635200\n",
      "Epoch 123, CIFAR-10 Batch 3:  0.6382\n",
      "Loss: 0.6204 Validation Accuracy: 0.638200\n",
      "Epoch 123, CIFAR-10 Batch 4:  0.63\n",
      "Loss: 0.6751 Validation Accuracy: 0.630000\n",
      "Epoch 123, CIFAR-10 Batch 5:  0.6414\n",
      "Loss: 0.6662 Validation Accuracy: 0.641400\n",
      "Epoch 124, CIFAR-10 Batch 1:  0.644\n",
      "Loss: 0.7088 Validation Accuracy: 0.644000\n",
      "Epoch 124, CIFAR-10 Batch 2:  0.6392\n",
      "Loss: 0.6556 Validation Accuracy: 0.639200\n",
      "Epoch 124, CIFAR-10 Batch 3:  0.6412\n",
      "Loss: 0.6206 Validation Accuracy: 0.641200\n",
      "Epoch 124, CIFAR-10 Batch 4:  0.6308\n",
      "Loss: 0.6617 Validation Accuracy: 0.630800\n",
      "Epoch 124, CIFAR-10 Batch 5:  0.637\n",
      "Loss: 0.6590 Validation Accuracy: 0.637000\n",
      "Epoch 125, CIFAR-10 Batch 1:  0.646\n",
      "Loss: 0.7066 Validation Accuracy: 0.646000\n",
      "Epoch 125, CIFAR-10 Batch 2:  0.6408\n",
      "Loss: 0.6814 Validation Accuracy: 0.640800\n",
      "Epoch 125, CIFAR-10 Batch 3:  0.64\n",
      "Loss: 0.6232 Validation Accuracy: 0.640000\n",
      "Epoch 125, CIFAR-10 Batch 4:  0.6344\n",
      "Loss: 0.6546 Validation Accuracy: 0.634400\n",
      "Epoch 125, CIFAR-10 Batch 5:  0.6362\n",
      "Loss: 0.6390 Validation Accuracy: 0.636200\n",
      "Epoch 126, CIFAR-10 Batch 1:  0.644\n",
      "Loss: 0.6965 Validation Accuracy: 0.644000\n",
      "Epoch 126, CIFAR-10 Batch 2:  0.6428\n",
      "Loss: 0.6987 Validation Accuracy: 0.642800\n",
      "Epoch 126, CIFAR-10 Batch 3:  0.6398\n",
      "Loss: 0.6211 Validation Accuracy: 0.639800\n",
      "Epoch 126, CIFAR-10 Batch 4:  0.6446\n",
      "Loss: 0.6474 Validation Accuracy: 0.644600\n",
      "Epoch 126, CIFAR-10 Batch 5:  0.645\n",
      "Loss: 0.6513 Validation Accuracy: 0.645000\n",
      "Epoch 127, CIFAR-10 Batch 1:  0.644\n",
      "Loss: 0.7012 Validation Accuracy: 0.644000\n",
      "Epoch 127, CIFAR-10 Batch 2:  0.6444\n",
      "Loss: 0.6660 Validation Accuracy: 0.644400\n",
      "Epoch 127, CIFAR-10 Batch 3:  0.6462\n",
      "Loss: 0.6055 Validation Accuracy: 0.646200\n",
      "Epoch 127, CIFAR-10 Batch 4:  0.6396\n",
      "Loss: 0.6529 Validation Accuracy: 0.639600\n",
      "Epoch 127, CIFAR-10 Batch 5:  0.638\n",
      "Loss: 0.6481 Validation Accuracy: 0.638000\n",
      "Epoch 128, CIFAR-10 Batch 1:  0.6466\n",
      "Loss: 0.6999 Validation Accuracy: 0.646600\n",
      "Epoch 128, CIFAR-10 Batch 2:  0.646\n",
      "Loss: 0.6582 Validation Accuracy: 0.646000\n",
      "Epoch 128, CIFAR-10 Batch 3:  0.6508\n",
      "Loss: 0.6070 Validation Accuracy: 0.650800\n",
      "Epoch 128, CIFAR-10 Batch 4:  0.6442\n",
      "Loss: 0.6403 Validation Accuracy: 0.644200\n",
      "Epoch 128, CIFAR-10 Batch 5:  0.6502\n",
      "Loss: 0.6562 Validation Accuracy: 0.650200\n",
      "Epoch 129, CIFAR-10 Batch 1:  0.6498\n",
      "Loss: 0.7036 Validation Accuracy: 0.649800\n",
      "Epoch 129, CIFAR-10 Batch 2:  0.6502\n",
      "Loss: 0.6597 Validation Accuracy: 0.650200\n",
      "Epoch 129, CIFAR-10 Batch 3:  0.6438\n",
      "Loss: 0.6030 Validation Accuracy: 0.643800\n",
      "Epoch 129, CIFAR-10 Batch 4:  0.63\n",
      "Loss: 0.6460 Validation Accuracy: 0.630000\n",
      "Epoch 129, CIFAR-10 Batch 5:  0.644\n",
      "Loss: 0.6497 Validation Accuracy: 0.644000\n",
      "Epoch 130, CIFAR-10 Batch 1:  0.6396\n",
      "Loss: 0.6993 Validation Accuracy: 0.639600\n",
      "Epoch 130, CIFAR-10 Batch 2:  0.6278\n",
      "Loss: 0.6644 Validation Accuracy: 0.627800\n",
      "Epoch 130, CIFAR-10 Batch 3:  0.6434\n",
      "Loss: 0.6020 Validation Accuracy: 0.643400\n",
      "Epoch 130, CIFAR-10 Batch 4:  0.6378\n",
      "Loss: 0.6592 Validation Accuracy: 0.637800\n",
      "Epoch 130, CIFAR-10 Batch 5:  0.6378\n",
      "Loss: 0.6459 Validation Accuracy: 0.637800\n",
      "Epoch 131, CIFAR-10 Batch 1:  0.646\n",
      "Loss: 0.6958 Validation Accuracy: 0.646000\n",
      "Epoch 131, CIFAR-10 Batch 2:  0.6468\n",
      "Loss: 0.6629 Validation Accuracy: 0.646800\n",
      "Epoch 131, CIFAR-10 Batch 3:  0.6404\n",
      "Loss: 0.6185 Validation Accuracy: 0.640400\n",
      "Epoch 131, CIFAR-10 Batch 4:  0.6254\n",
      "Loss: 0.6810 Validation Accuracy: 0.625400\n",
      "Epoch 131, CIFAR-10 Batch 5:  0.6434\n",
      "Loss: 0.6761 Validation Accuracy: 0.643400\n",
      "Epoch 132, CIFAR-10 Batch 1:  0.6418\n",
      "Loss: 0.7130 Validation Accuracy: 0.641800\n",
      "Epoch 132, CIFAR-10 Batch 2:  0.6526\n",
      "Loss: 0.6590 Validation Accuracy: 0.652600\n",
      "Epoch 132, CIFAR-10 Batch 3:  0.6422\n",
      "Loss: 0.6053 Validation Accuracy: 0.642200\n",
      "Epoch 132, CIFAR-10 Batch 4:  0.6326\n",
      "Loss: 0.6524 Validation Accuracy: 0.632600\n",
      "Epoch 132, CIFAR-10 Batch 5:  0.6406\n",
      "Loss: 0.6522 Validation Accuracy: 0.640600\n",
      "Epoch 133, CIFAR-10 Batch 1:  0.6526\n",
      "Loss: 0.6837 Validation Accuracy: 0.652600\n",
      "Epoch 133, CIFAR-10 Batch 2:  0.6492\n",
      "Loss: 0.6525 Validation Accuracy: 0.649200\n",
      "Epoch 133, CIFAR-10 Batch 3:  0.6454\n",
      "Loss: 0.6022 Validation Accuracy: 0.645400\n",
      "Epoch 133, CIFAR-10 Batch 4:  0.633\n",
      "Loss: 0.6337 Validation Accuracy: 0.633000\n",
      "Epoch 133, CIFAR-10 Batch 5:  0.6502\n",
      "Loss: 0.6477 Validation Accuracy: 0.650200\n",
      "Epoch 134, CIFAR-10 Batch 1:  0.6474\n",
      "Loss: 0.6946 Validation Accuracy: 0.647400\n",
      "Epoch 134, CIFAR-10 Batch 2:  0.6466\n",
      "Loss: 0.6581 Validation Accuracy: 0.646600\n",
      "Epoch 134, CIFAR-10 Batch 3:  0.6424\n",
      "Loss: 0.5871 Validation Accuracy: 0.642400\n",
      "Epoch 134, CIFAR-10 Batch 4:  0.6268\n",
      "Loss: 0.6489 Validation Accuracy: 0.626800\n",
      "Epoch 134, CIFAR-10 Batch 5:  0.6406\n",
      "Loss: 0.6667 Validation Accuracy: 0.640600\n",
      "Epoch 135, CIFAR-10 Batch 1:  0.6492\n",
      "Loss: 0.6922 Validation Accuracy: 0.649200\n",
      "Epoch 135, CIFAR-10 Batch 2:  0.647\n",
      "Loss: 0.6502 Validation Accuracy: 0.647000\n",
      "Epoch 135, CIFAR-10 Batch 3:  0.6372\n",
      "Loss: 0.6370 Validation Accuracy: 0.637200\n",
      "Epoch 135, CIFAR-10 Batch 4:  0.637\n",
      "Loss: 0.6645 Validation Accuracy: 0.637000\n",
      "Epoch 135, CIFAR-10 Batch 5:  0.6492\n",
      "Loss: 0.6366 Validation Accuracy: 0.649200\n",
      "Epoch 136, CIFAR-10 Batch 1:  0.646\n",
      "Loss: 0.6984 Validation Accuracy: 0.646000\n",
      "Epoch 136, CIFAR-10 Batch 2:  0.652\n",
      "Loss: 0.6747 Validation Accuracy: 0.652000\n",
      "Epoch 136, CIFAR-10 Batch 3:  0.6466\n",
      "Loss: 0.6010 Validation Accuracy: 0.646600\n",
      "Epoch 136, CIFAR-10 Batch 4:  0.6484\n",
      "Loss: 0.6232 Validation Accuracy: 0.648400\n",
      "Epoch 136, CIFAR-10 Batch 5:  0.6474\n",
      "Loss: 0.6386 Validation Accuracy: 0.647400\n",
      "Epoch 137, CIFAR-10 Batch 1:  0.6442\n",
      "Loss: 0.6824 Validation Accuracy: 0.644200\n",
      "Epoch 137, CIFAR-10 Batch 2:  0.6428\n",
      "Loss: 0.6451 Validation Accuracy: 0.642800\n",
      "Epoch 137, CIFAR-10 Batch 3:  0.65\n",
      "Loss: 0.5882 Validation Accuracy: 0.650000\n",
      "Epoch 137, CIFAR-10 Batch 4:  0.6428\n",
      "Loss: 0.6313 Validation Accuracy: 0.642800\n",
      "Epoch 137, CIFAR-10 Batch 5:  0.6494\n",
      "Loss: 0.6417 Validation Accuracy: 0.649400\n",
      "Epoch 138, CIFAR-10 Batch 1:  0.6488\n",
      "Loss: 0.6859 Validation Accuracy: 0.648800\n",
      "Epoch 138, CIFAR-10 Batch 2:  0.6516\n",
      "Loss: 0.6506 Validation Accuracy: 0.651600\n",
      "Epoch 138, CIFAR-10 Batch 3:  0.651\n",
      "Loss: 0.6040 Validation Accuracy: 0.651000\n",
      "Epoch 138, CIFAR-10 Batch 4:  0.6458\n",
      "Loss: 0.6192 Validation Accuracy: 0.645800\n",
      "Epoch 138, CIFAR-10 Batch 5:  0.6458\n",
      "Loss: 0.6355 Validation Accuracy: 0.645800\n",
      "Epoch 139, CIFAR-10 Batch 1:  0.647\n",
      "Loss: 0.6873 Validation Accuracy: 0.647000\n",
      "Epoch 139, CIFAR-10 Batch 2:  0.6522\n",
      "Loss: 0.6554 Validation Accuracy: 0.652200\n",
      "Epoch 139, CIFAR-10 Batch 3:  0.6498\n",
      "Loss: 0.6035 Validation Accuracy: 0.649800\n",
      "Epoch 139, CIFAR-10 Batch 4:  0.6446\n",
      "Loss: 0.6209 Validation Accuracy: 0.644600\n",
      "Epoch 139, CIFAR-10 Batch 5:  0.645\n",
      "Loss: 0.6345 Validation Accuracy: 0.645000\n",
      "Epoch 140, CIFAR-10 Batch 1:  0.6516\n",
      "Loss: 0.6893 Validation Accuracy: 0.651600\n",
      "Epoch 140, CIFAR-10 Batch 2:  0.6448\n",
      "Loss: 0.6435 Validation Accuracy: 0.644800\n",
      "Epoch 140, CIFAR-10 Batch 3:  0.6434\n",
      "Loss: 0.5927 Validation Accuracy: 0.643400\n",
      "Epoch 140, CIFAR-10 Batch 4:  0.6426\n",
      "Loss: 0.6240 Validation Accuracy: 0.642600\n",
      "Epoch 140, CIFAR-10 Batch 5:  0.6518\n",
      "Loss: 0.6386 Validation Accuracy: 0.651800\n",
      "Epoch 141, CIFAR-10 Batch 1:  0.6454\n",
      "Loss: 0.6866 Validation Accuracy: 0.645400\n",
      "Epoch 141, CIFAR-10 Batch 2:  0.645\n",
      "Loss: 0.6357 Validation Accuracy: 0.645000\n",
      "Epoch 141, CIFAR-10 Batch 3:  0.6478\n",
      "Loss: 0.6030 Validation Accuracy: 0.647800\n",
      "Epoch 141, CIFAR-10 Batch 4:  0.6392\n",
      "Loss: 0.6160 Validation Accuracy: 0.639200\n",
      "Epoch 141, CIFAR-10 Batch 5:  0.6522\n",
      "Loss: 0.6184 Validation Accuracy: 0.652200\n",
      "Epoch 142, CIFAR-10 Batch 1:  0.6432\n",
      "Loss: 0.6694 Validation Accuracy: 0.643200\n",
      "Epoch 142, CIFAR-10 Batch 2:  0.6414\n",
      "Loss: 0.6602 Validation Accuracy: 0.641400\n",
      "Epoch 142, CIFAR-10 Batch 3:  0.645\n",
      "Loss: 0.5831 Validation Accuracy: 0.645000\n",
      "Epoch 142, CIFAR-10 Batch 4:  0.655\n",
      "Loss: 0.6091 Validation Accuracy: 0.655000\n",
      "Epoch 142, CIFAR-10 Batch 5:  0.6504\n",
      "Loss: 0.6199 Validation Accuracy: 0.650400\n",
      "Epoch 143, CIFAR-10 Batch 1:  0.6534\n",
      "Loss: 0.6705 Validation Accuracy: 0.653400\n",
      "Epoch 143, CIFAR-10 Batch 2:  0.6432\n",
      "Loss: 0.6273 Validation Accuracy: 0.643200\n",
      "Epoch 143, CIFAR-10 Batch 3:  0.6464\n",
      "Loss: 0.5933 Validation Accuracy: 0.646400\n",
      "Epoch 143, CIFAR-10 Batch 4:  0.6484\n",
      "Loss: 0.6075 Validation Accuracy: 0.648400\n",
      "Epoch 143, CIFAR-10 Batch 5:  0.6468\n",
      "Loss: 0.6127 Validation Accuracy: 0.646800\n",
      "Epoch 144, CIFAR-10 Batch 1:  0.638\n",
      "Loss: 0.6835 Validation Accuracy: 0.638000\n",
      "Epoch 144, CIFAR-10 Batch 2:  0.6534\n",
      "Loss: 0.6335 Validation Accuracy: 0.653400\n",
      "Epoch 144, CIFAR-10 Batch 3:  0.654\n",
      "Loss: 0.5808 Validation Accuracy: 0.654000\n",
      "Epoch 144, CIFAR-10 Batch 4:  0.6372\n",
      "Loss: 0.5986 Validation Accuracy: 0.637200\n",
      "Epoch 144, CIFAR-10 Batch 5:  0.6436\n",
      "Loss: 0.6476 Validation Accuracy: 0.643600\n",
      "Epoch 145, CIFAR-10 Batch 1:  0.6504\n",
      "Loss: 0.6677 Validation Accuracy: 0.650400\n",
      "Epoch 145, CIFAR-10 Batch 2:  0.6486\n",
      "Loss: 0.6527 Validation Accuracy: 0.648600\n",
      "Epoch 145, CIFAR-10 Batch 3:  0.65\n",
      "Loss: 0.5921 Validation Accuracy: 0.650000\n",
      "Epoch 145, CIFAR-10 Batch 4:  0.6298\n",
      "Loss: 0.6467 Validation Accuracy: 0.629800\n",
      "Epoch 145, CIFAR-10 Batch 5:  0.642\n",
      "Loss: 0.6227 Validation Accuracy: 0.642000\n",
      "Epoch 146, CIFAR-10 Batch 1:  0.646\n",
      "Loss: 0.6550 Validation Accuracy: 0.646000\n",
      "Epoch 146, CIFAR-10 Batch 2:  0.6598\n",
      "Loss: 0.6374 Validation Accuracy: 0.659800\n",
      "Epoch 146, CIFAR-10 Batch 3:  0.6376\n",
      "Loss: 0.5967 Validation Accuracy: 0.637600\n",
      "Epoch 146, CIFAR-10 Batch 4:  0.6344\n",
      "Loss: 0.6205 Validation Accuracy: 0.634400\n",
      "Epoch 146, CIFAR-10 Batch 5:  0.6474\n",
      "Loss: 0.6374 Validation Accuracy: 0.647400\n",
      "Epoch 147, CIFAR-10 Batch 1:  0.6522\n",
      "Loss: 0.6703 Validation Accuracy: 0.652200\n",
      "Epoch 147, CIFAR-10 Batch 2:  0.645\n",
      "Loss: 0.6392 Validation Accuracy: 0.645000\n",
      "Epoch 147, CIFAR-10 Batch 3:  0.643\n",
      "Loss: 0.5961 Validation Accuracy: 0.643000\n",
      "Epoch 147, CIFAR-10 Batch 4:  0.6486\n",
      "Loss: 0.6018 Validation Accuracy: 0.648600\n",
      "Epoch 147, CIFAR-10 Batch 5:  0.6452\n",
      "Loss: 0.6284 Validation Accuracy: 0.645200\n",
      "Epoch 148, CIFAR-10 Batch 1:  0.6466\n",
      "Loss: 0.6506 Validation Accuracy: 0.646600\n",
      "Epoch 148, CIFAR-10 Batch 2:  0.6478\n",
      "Loss: 0.6367 Validation Accuracy: 0.647800\n",
      "Epoch 148, CIFAR-10 Batch 3:  0.6448\n",
      "Loss: 0.5582 Validation Accuracy: 0.644800\n",
      "Epoch 148, CIFAR-10 Batch 4:  0.646\n",
      "Loss: 0.5933 Validation Accuracy: 0.646000\n",
      "Epoch 148, CIFAR-10 Batch 5:  0.6418\n",
      "Loss: 0.6161 Validation Accuracy: 0.641800\n",
      "Epoch 149, CIFAR-10 Batch 1:  0.6456\n",
      "Loss: 0.6658 Validation Accuracy: 0.645600\n",
      "Epoch 149, CIFAR-10 Batch 2:  0.6472\n",
      "Loss: 0.6355 Validation Accuracy: 0.647200\n",
      "Epoch 149, CIFAR-10 Batch 3:  0.6532\n",
      "Loss: 0.5777 Validation Accuracy: 0.653200\n",
      "Epoch 149, CIFAR-10 Batch 4:  0.6476\n",
      "Loss: 0.6167 Validation Accuracy: 0.647600\n",
      "Epoch 149, CIFAR-10 Batch 5:  0.6362\n",
      "Loss: 0.6359 Validation Accuracy: 0.636200\n",
      "Epoch 150, CIFAR-10 Batch 1:  0.6496\n",
      "Loss: 0.6895 Validation Accuracy: 0.649600\n",
      "Epoch 150, CIFAR-10 Batch 2:  0.6452\n",
      "Loss: 0.6417 Validation Accuracy: 0.645200\n",
      "Epoch 150, CIFAR-10 Batch 3:  0.6518\n",
      "Loss: 0.5799 Validation Accuracy: 0.651800\n",
      "Epoch 150, CIFAR-10 Batch 4:  0.6436\n",
      "Loss: 0.6086 Validation Accuracy: 0.643600\n",
      "Epoch 150, CIFAR-10 Batch 5:  0.6326\n",
      "Loss: 0.6535 Validation Accuracy: 0.632600\n",
      "Epoch 151, CIFAR-10 Batch 1:  0.6442\n",
      "Loss: 0.6798 Validation Accuracy: 0.644200\n",
      "Epoch 151, CIFAR-10 Batch 2:  0.6454\n",
      "Loss: 0.6555 Validation Accuracy: 0.645400\n",
      "Epoch 151, CIFAR-10 Batch 3:  0.6542\n",
      "Loss: 0.5758 Validation Accuracy: 0.654200\n",
      "Epoch 151, CIFAR-10 Batch 4:  0.629\n",
      "Loss: 0.6313 Validation Accuracy: 0.629000\n",
      "Epoch 151, CIFAR-10 Batch 5:  0.6492\n",
      "Loss: 0.6339 Validation Accuracy: 0.649200\n",
      "Epoch 152, CIFAR-10 Batch 1:  0.6406\n",
      "Loss: 0.6845 Validation Accuracy: 0.640600\n",
      "Epoch 152, CIFAR-10 Batch 2:  0.6364\n",
      "Loss: 0.6670 Validation Accuracy: 0.636400\n",
      "Epoch 152, CIFAR-10 Batch 3:  0.6468\n",
      "Loss: 0.5735 Validation Accuracy: 0.646800\n",
      "Epoch 152, CIFAR-10 Batch 4:  0.6436\n",
      "Loss: 0.5930 Validation Accuracy: 0.643600\n",
      "Epoch 152, CIFAR-10 Batch 5:  0.6274\n",
      "Loss: 0.6584 Validation Accuracy: 0.627400\n",
      "Epoch 153, CIFAR-10 Batch 1:  0.6394\n",
      "Loss: 0.6891 Validation Accuracy: 0.639400\n",
      "Epoch 153, CIFAR-10 Batch 2:  0.6448\n",
      "Loss: 0.6567 Validation Accuracy: 0.644800\n",
      "Epoch 153, CIFAR-10 Batch 3:  0.6348\n",
      "Loss: 0.5875 Validation Accuracy: 0.634800\n",
      "Epoch 153, CIFAR-10 Batch 4:  0.6226\n",
      "Loss: 0.6254 Validation Accuracy: 0.622600\n",
      "Epoch 153, CIFAR-10 Batch 5:  0.6484\n",
      "Loss: 0.6217 Validation Accuracy: 0.648400\n",
      "Epoch 154, CIFAR-10 Batch 1:  0.654\n",
      "Loss: 0.6672 Validation Accuracy: 0.654000\n",
      "Epoch 154, CIFAR-10 Batch 2:  0.6486\n",
      "Loss: 0.6397 Validation Accuracy: 0.648600\n",
      "Epoch 154, CIFAR-10 Batch 3:  0.6458\n",
      "Loss: 0.5923 Validation Accuracy: 0.645800\n",
      "Epoch 154, CIFAR-10 Batch 4:  0.6426\n",
      "Loss: 0.6092 Validation Accuracy: 0.642600\n",
      "Epoch 154, CIFAR-10 Batch 5:  0.6358\n",
      "Loss: 0.6249 Validation Accuracy: 0.635800\n",
      "Epoch 155, CIFAR-10 Batch 1:  0.6366\n",
      "Loss: 0.6842 Validation Accuracy: 0.636600\n",
      "Epoch 155, CIFAR-10 Batch 2:  0.6452\n",
      "Loss: 0.6597 Validation Accuracy: 0.645200\n",
      "Epoch 155, CIFAR-10 Batch 3:  0.6462\n",
      "Loss: 0.5766 Validation Accuracy: 0.646200\n",
      "Epoch 155, CIFAR-10 Batch 4:  0.6376\n",
      "Loss: 0.6135 Validation Accuracy: 0.637600\n",
      "Epoch 155, CIFAR-10 Batch 5:  0.6436\n",
      "Loss: 0.6238 Validation Accuracy: 0.643600\n",
      "Epoch 156, CIFAR-10 Batch 1:  0.6462\n",
      "Loss: 0.6662 Validation Accuracy: 0.646200\n",
      "Epoch 156, CIFAR-10 Batch 2:  0.6498\n",
      "Loss: 0.6346 Validation Accuracy: 0.649800\n",
      "Epoch 156, CIFAR-10 Batch 3:  0.6402\n",
      "Loss: 0.5705 Validation Accuracy: 0.640200\n",
      "Epoch 156, CIFAR-10 Batch 4:  0.6366\n",
      "Loss: 0.5999 Validation Accuracy: 0.636600\n",
      "Epoch 156, CIFAR-10 Batch 5:  0.646\n",
      "Loss: 0.6069 Validation Accuracy: 0.646000\n",
      "Epoch 157, CIFAR-10 Batch 1:  0.6578\n",
      "Loss: 0.6584 Validation Accuracy: 0.657800\n",
      "Epoch 157, CIFAR-10 Batch 2:  0.6484\n",
      "Loss: 0.6431 Validation Accuracy: 0.648400\n",
      "Epoch 157, CIFAR-10 Batch 3:  0.648\n",
      "Loss: 0.5632 Validation Accuracy: 0.648000\n",
      "Epoch 157, CIFAR-10 Batch 4:  0.6302\n",
      "Loss: 0.6156 Validation Accuracy: 0.630200\n",
      "Epoch 157, CIFAR-10 Batch 5:  0.648\n",
      "Loss: 0.6050 Validation Accuracy: 0.648000\n",
      "Epoch 158, CIFAR-10 Batch 1:  0.6512\n",
      "Loss: 0.6644 Validation Accuracy: 0.651200\n",
      "Epoch 158, CIFAR-10 Batch 2:  0.6434\n",
      "Loss: 0.6643 Validation Accuracy: 0.643400\n",
      "Epoch 158, CIFAR-10 Batch 3:  0.653\n",
      "Loss: 0.5622 Validation Accuracy: 0.653000\n",
      "Epoch 158, CIFAR-10 Batch 4:  0.6402\n",
      "Loss: 0.5920 Validation Accuracy: 0.640200\n",
      "Epoch 158, CIFAR-10 Batch 5:  0.6348\n",
      "Loss: 0.6272 Validation Accuracy: 0.634800\n",
      "Epoch 159, CIFAR-10 Batch 1:  0.6434\n",
      "Loss: 0.6414 Validation Accuracy: 0.643400\n",
      "Epoch 159, CIFAR-10 Batch 2:  0.6516\n",
      "Loss: 0.6169 Validation Accuracy: 0.651600\n",
      "Epoch 159, CIFAR-10 Batch 3:  0.6398\n",
      "Loss: 0.5798 Validation Accuracy: 0.639800\n",
      "Epoch 159, CIFAR-10 Batch 4:  0.6418\n",
      "Loss: 0.5792 Validation Accuracy: 0.641800\n",
      "Epoch 159, CIFAR-10 Batch 5:  0.6434\n",
      "Loss: 0.6202 Validation Accuracy: 0.643400\n",
      "Epoch 160, CIFAR-10 Batch 1:  0.6486\n",
      "Loss: 0.6528 Validation Accuracy: 0.648600\n",
      "Epoch 160, CIFAR-10 Batch 2:  0.6468\n",
      "Loss: 0.6197 Validation Accuracy: 0.646800\n",
      "Epoch 160, CIFAR-10 Batch 3:  0.6452\n",
      "Loss: 0.5771 Validation Accuracy: 0.645200\n",
      "Epoch 160, CIFAR-10 Batch 4:  0.6398\n",
      "Loss: 0.6059 Validation Accuracy: 0.639800\n",
      "Epoch 160, CIFAR-10 Batch 5:  0.6364\n",
      "Loss: 0.6152 Validation Accuracy: 0.636400\n",
      "Epoch 161, CIFAR-10 Batch 1:  0.6522\n",
      "Loss: 0.6667 Validation Accuracy: 0.652200\n",
      "Epoch 161, CIFAR-10 Batch 2:  0.6452\n",
      "Loss: 0.6230 Validation Accuracy: 0.645200\n",
      "Epoch 161, CIFAR-10 Batch 3:  0.647\n",
      "Loss: 0.5534 Validation Accuracy: 0.647000\n",
      "Epoch 161, CIFAR-10 Batch 4:  0.638\n",
      "Loss: 0.6135 Validation Accuracy: 0.638000\n",
      "Epoch 161, CIFAR-10 Batch 5:  0.6514\n",
      "Loss: 0.5966 Validation Accuracy: 0.651400\n",
      "Epoch 162, CIFAR-10 Batch 1:  0.655\n",
      "Loss: 0.6479 Validation Accuracy: 0.655000\n",
      "Epoch 162, CIFAR-10 Batch 2:  0.6536\n",
      "Loss: 0.6132 Validation Accuracy: 0.653600\n",
      "Epoch 162, CIFAR-10 Batch 3:  0.6476\n",
      "Loss: 0.5671 Validation Accuracy: 0.647600\n",
      "Epoch 162, CIFAR-10 Batch 4:  0.6308\n",
      "Loss: 0.6255 Validation Accuracy: 0.630800\n",
      "Epoch 162, CIFAR-10 Batch 5:  0.642\n",
      "Loss: 0.6086 Validation Accuracy: 0.642000\n",
      "Epoch 163, CIFAR-10 Batch 1:  0.6352\n",
      "Loss: 0.6440 Validation Accuracy: 0.635200\n",
      "Epoch 163, CIFAR-10 Batch 2:  0.6354\n",
      "Loss: 0.6350 Validation Accuracy: 0.635400\n",
      "Epoch 163, CIFAR-10 Batch 3:  0.6554\n",
      "Loss: 0.5477 Validation Accuracy: 0.655400\n",
      "Epoch 163, CIFAR-10 Batch 4:  0.623\n",
      "Loss: 0.6039 Validation Accuracy: 0.623000\n",
      "Epoch 163, CIFAR-10 Batch 5:  0.637\n",
      "Loss: 0.6170 Validation Accuracy: 0.637000\n",
      "Epoch 164, CIFAR-10 Batch 1:  0.6422\n",
      "Loss: 0.6685 Validation Accuracy: 0.642200\n",
      "Epoch 164, CIFAR-10 Batch 2:  0.6454\n",
      "Loss: 0.6373 Validation Accuracy: 0.645400\n",
      "Epoch 164, CIFAR-10 Batch 3:  0.6516\n",
      "Loss: 0.5410 Validation Accuracy: 0.651600\n",
      "Epoch 164, CIFAR-10 Batch 4:  0.6364\n",
      "Loss: 0.5769 Validation Accuracy: 0.636400\n",
      "Epoch 164, CIFAR-10 Batch 5:  0.6436\n",
      "Loss: 0.6019 Validation Accuracy: 0.643600\n",
      "Epoch 165, CIFAR-10 Batch 1:  0.6458\n",
      "Loss: 0.6370 Validation Accuracy: 0.645800\n",
      "Epoch 165, CIFAR-10 Batch 2:  0.6492\n",
      "Loss: 0.6148 Validation Accuracy: 0.649200\n",
      "Epoch 165, CIFAR-10 Batch 3:  0.65\n",
      "Loss: 0.5699 Validation Accuracy: 0.650000\n",
      "Epoch 165, CIFAR-10 Batch 4:  0.6382\n",
      "Loss: 0.6115 Validation Accuracy: 0.638200\n",
      "Epoch 165, CIFAR-10 Batch 5:  0.6464\n",
      "Loss: 0.6051 Validation Accuracy: 0.646400\n",
      "Epoch 166, CIFAR-10 Batch 1:  0.6484\n",
      "Loss: 0.6415 Validation Accuracy: 0.648400\n",
      "Epoch 166, CIFAR-10 Batch 2:  0.6466\n",
      "Loss: 0.6318 Validation Accuracy: 0.646600\n",
      "Epoch 166, CIFAR-10 Batch 3:  0.6506\n",
      "Loss: 0.5655 Validation Accuracy: 0.650600\n",
      "Epoch 166, CIFAR-10 Batch 4:  0.64\n",
      "Loss: 0.5889 Validation Accuracy: 0.640000\n",
      "Epoch 166, CIFAR-10 Batch 5:  0.6378\n",
      "Loss: 0.6283 Validation Accuracy: 0.637800\n",
      "Epoch 167, CIFAR-10 Batch 1:  0.6462\n",
      "Loss: 0.6451 Validation Accuracy: 0.646200\n",
      "Epoch 167, CIFAR-10 Batch 2:  0.6512\n",
      "Loss: 0.6208 Validation Accuracy: 0.651200\n",
      "Epoch 167, CIFAR-10 Batch 3:  0.6448\n",
      "Loss: 0.5756 Validation Accuracy: 0.644800\n",
      "Epoch 167, CIFAR-10 Batch 4:  0.6308\n",
      "Loss: 0.5944 Validation Accuracy: 0.630800\n",
      "Epoch 167, CIFAR-10 Batch 5:  0.641\n",
      "Loss: 0.5873 Validation Accuracy: 0.641000\n",
      "Epoch 168, CIFAR-10 Batch 1:  0.646\n",
      "Loss: 0.6337 Validation Accuracy: 0.646000\n",
      "Epoch 168, CIFAR-10 Batch 2:  0.6576\n",
      "Loss: 0.6075 Validation Accuracy: 0.657600\n",
      "Epoch 168, CIFAR-10 Batch 3:  0.649\n",
      "Loss: 0.5548 Validation Accuracy: 0.649000\n",
      "Epoch 168, CIFAR-10 Batch 4:  0.6378\n",
      "Loss: 0.5930 Validation Accuracy: 0.637800\n",
      "Epoch 168, CIFAR-10 Batch 5:  0.6392\n",
      "Loss: 0.6012 Validation Accuracy: 0.639200\n",
      "Epoch 169, CIFAR-10 Batch 1:  0.6502\n",
      "Loss: 0.6272 Validation Accuracy: 0.650200\n",
      "Epoch 169, CIFAR-10 Batch 2:  0.64\n",
      "Loss: 0.6258 Validation Accuracy: 0.640000\n",
      "Epoch 169, CIFAR-10 Batch 3:  0.661\n",
      "Loss: 0.5599 Validation Accuracy: 0.661000\n",
      "Epoch 169, CIFAR-10 Batch 4:  0.6288\n",
      "Loss: 0.5930 Validation Accuracy: 0.628800\n",
      "Epoch 169, CIFAR-10 Batch 5:  0.6352\n",
      "Loss: 0.6018 Validation Accuracy: 0.635200\n",
      "Epoch 170, CIFAR-10 Batch 1:  0.6518\n",
      "Loss: 0.6353 Validation Accuracy: 0.651800\n",
      "Epoch 170, CIFAR-10 Batch 2:  0.6466\n",
      "Loss: 0.6366 Validation Accuracy: 0.646600\n",
      "Epoch 170, CIFAR-10 Batch 3:  0.6498\n",
      "Loss: 0.5663 Validation Accuracy: 0.649800\n",
      "Epoch 170, CIFAR-10 Batch 4:  0.6306\n",
      "Loss: 0.6058 Validation Accuracy: 0.630600\n",
      "Epoch 170, CIFAR-10 Batch 5:  0.653\n",
      "Loss: 0.5935 Validation Accuracy: 0.653000\n",
      "Epoch 171, CIFAR-10 Batch 1:  0.65\n",
      "Loss: 0.6350 Validation Accuracy: 0.650000\n",
      "Epoch 171, CIFAR-10 Batch 2:  0.6574\n",
      "Loss: 0.6113 Validation Accuracy: 0.657400\n",
      "Epoch 171, CIFAR-10 Batch 3:  0.651\n",
      "Loss: 0.5559 Validation Accuracy: 0.651000\n",
      "Epoch 171, CIFAR-10 Batch 4:  0.636\n",
      "Loss: 0.5963 Validation Accuracy: 0.636000\n",
      "Epoch 171, CIFAR-10 Batch 5:  0.6358\n",
      "Loss: 0.6179 Validation Accuracy: 0.635800\n",
      "Epoch 172, CIFAR-10 Batch 1:  0.643\n",
      "Loss: 0.6389 Validation Accuracy: 0.643000\n",
      "Epoch 172, CIFAR-10 Batch 2:  0.6504\n",
      "Loss: 0.6026 Validation Accuracy: 0.650400\n",
      "Epoch 172, CIFAR-10 Batch 3:  0.6458\n",
      "Loss: 0.5444 Validation Accuracy: 0.645800\n",
      "Epoch 172, CIFAR-10 Batch 4:  0.6392\n",
      "Loss: 0.5975 Validation Accuracy: 0.639200\n",
      "Epoch 172, CIFAR-10 Batch 5:  0.644\n",
      "Loss: 0.6034 Validation Accuracy: 0.644000\n",
      "Epoch 173, CIFAR-10 Batch 1:  0.6522\n",
      "Loss: 0.6483 Validation Accuracy: 0.652200\n",
      "Epoch 173, CIFAR-10 Batch 2:  0.6504\n",
      "Loss: 0.6334 Validation Accuracy: 0.650400\n",
      "Epoch 173, CIFAR-10 Batch 3:  0.6566\n",
      "Loss: 0.5570 Validation Accuracy: 0.656600\n",
      "Epoch 173, CIFAR-10 Batch 4:  0.6388\n",
      "Loss: 0.5787 Validation Accuracy: 0.638800\n",
      "Epoch 173, CIFAR-10 Batch 5:  0.6434\n",
      "Loss: 0.6029 Validation Accuracy: 0.643400\n",
      "Epoch 174, CIFAR-10 Batch 1:  0.6486\n",
      "Loss: 0.6308 Validation Accuracy: 0.648600\n",
      "Epoch 174, CIFAR-10 Batch 2:  0.6478\n",
      "Loss: 0.6177 Validation Accuracy: 0.647800\n",
      "Epoch 174, CIFAR-10 Batch 3:  0.6606\n",
      "Loss: 0.5434 Validation Accuracy: 0.660600\n",
      "Epoch 174, CIFAR-10 Batch 4:  0.6226\n",
      "Loss: 0.5953 Validation Accuracy: 0.622600\n",
      "Epoch 174, CIFAR-10 Batch 5:  0.6394\n",
      "Loss: 0.5940 Validation Accuracy: 0.639400\n",
      "Epoch 175, CIFAR-10 Batch 1:  0.6446\n",
      "Loss: 0.6260 Validation Accuracy: 0.644600\n",
      "Epoch 175, CIFAR-10 Batch 2:  0.636\n",
      "Loss: 0.6238 Validation Accuracy: 0.636000\n",
      "Epoch 175, CIFAR-10 Batch 3:  0.6468\n",
      "Loss: 0.5495 Validation Accuracy: 0.646800\n",
      "Epoch 175, CIFAR-10 Batch 4:  0.6354\n",
      "Loss: 0.5751 Validation Accuracy: 0.635400\n",
      "Epoch 175, CIFAR-10 Batch 5:  0.6446\n",
      "Loss: 0.5868 Validation Accuracy: 0.644600\n",
      "Epoch 176, CIFAR-10 Batch 1:  0.6456\n",
      "Loss: 0.6237 Validation Accuracy: 0.645600\n",
      "Epoch 176, CIFAR-10 Batch 2:  0.648\n",
      "Loss: 0.6246 Validation Accuracy: 0.648000\n",
      "Epoch 176, CIFAR-10 Batch 3:  0.6466\n",
      "Loss: 0.5478 Validation Accuracy: 0.646600\n",
      "Epoch 176, CIFAR-10 Batch 4:  0.6464\n",
      "Loss: 0.5855 Validation Accuracy: 0.646400\n",
      "Epoch 176, CIFAR-10 Batch 5:  0.6436\n",
      "Loss: 0.5962 Validation Accuracy: 0.643600\n",
      "Epoch 177, CIFAR-10 Batch 1:  0.6396\n",
      "Loss: 0.6262 Validation Accuracy: 0.639600\n",
      "Epoch 177, CIFAR-10 Batch 2:  0.643\n",
      "Loss: 0.6109 Validation Accuracy: 0.643000\n",
      "Epoch 177, CIFAR-10 Batch 3:  0.6526\n",
      "Loss: 0.5355 Validation Accuracy: 0.652600\n",
      "Epoch 177, CIFAR-10 Batch 4:  0.6356\n",
      "Loss: 0.5773 Validation Accuracy: 0.635600\n",
      "Epoch 177, CIFAR-10 Batch 5:  0.6304\n",
      "Loss: 0.6241 Validation Accuracy: 0.630400\n",
      "Epoch 178, CIFAR-10 Batch 1:  0.6522\n",
      "Loss: 0.6483 Validation Accuracy: 0.652200\n",
      "Epoch 178, CIFAR-10 Batch 2:  0.6488\n",
      "Loss: 0.6142 Validation Accuracy: 0.648800\n",
      "Epoch 178, CIFAR-10 Batch 3:  0.6576\n",
      "Loss: 0.5216 Validation Accuracy: 0.657600\n",
      "Epoch 178, CIFAR-10 Batch 4:  0.632\n",
      "Loss: 0.5864 Validation Accuracy: 0.632000\n",
      "Epoch 178, CIFAR-10 Batch 5:  0.6326\n",
      "Loss: 0.6246 Validation Accuracy: 0.632600\n",
      "Epoch 179, CIFAR-10 Batch 1:  0.6508\n",
      "Loss: 0.6362 Validation Accuracy: 0.650800\n",
      "Epoch 179, CIFAR-10 Batch 2:  0.646\n",
      "Loss: 0.6070 Validation Accuracy: 0.646000\n",
      "Epoch 179, CIFAR-10 Batch 3:  0.6534\n",
      "Loss: 0.5387 Validation Accuracy: 0.653400\n",
      "Epoch 179, CIFAR-10 Batch 4:  0.615\n",
      "Loss: 0.6020 Validation Accuracy: 0.615000\n",
      "Epoch 179, CIFAR-10 Batch 5:  0.6418\n",
      "Loss: 0.6152 Validation Accuracy: 0.641800\n",
      "Epoch 180, CIFAR-10 Batch 1:  0.651\n",
      "Loss: 0.6298 Validation Accuracy: 0.651000\n",
      "Epoch 180, CIFAR-10 Batch 2:  0.65\n",
      "Loss: 0.6039 Validation Accuracy: 0.650000\n",
      "Epoch 180, CIFAR-10 Batch 3:  0.6502\n",
      "Loss: 0.5364 Validation Accuracy: 0.650200\n",
      "Epoch 180, CIFAR-10 Batch 4:  0.6338\n",
      "Loss: 0.5827 Validation Accuracy: 0.633800\n",
      "Epoch 180, CIFAR-10 Batch 5:  0.6512\n",
      "Loss: 0.5878 Validation Accuracy: 0.651200\n",
      "Epoch 181, CIFAR-10 Batch 1:  0.6474\n",
      "Loss: 0.6131 Validation Accuracy: 0.647400\n",
      "Epoch 181, CIFAR-10 Batch 2:  0.6492\n",
      "Loss: 0.6064 Validation Accuracy: 0.649200\n",
      "Epoch 181, CIFAR-10 Batch 3:  0.6554\n",
      "Loss: 0.5250 Validation Accuracy: 0.655400\n",
      "Epoch 181, CIFAR-10 Batch 4:  0.6412\n",
      "Loss: 0.5582 Validation Accuracy: 0.641200\n",
      "Epoch 181, CIFAR-10 Batch 5:  0.6378\n",
      "Loss: 0.5866 Validation Accuracy: 0.637800\n",
      "Epoch 182, CIFAR-10 Batch 1:  0.6478\n",
      "Loss: 0.6170 Validation Accuracy: 0.647800\n",
      "Epoch 182, CIFAR-10 Batch 2:  0.651\n",
      "Loss: 0.5980 Validation Accuracy: 0.651000\n",
      "Epoch 182, CIFAR-10 Batch 3:  0.649\n",
      "Loss: 0.5357 Validation Accuracy: 0.649000\n",
      "Epoch 182, CIFAR-10 Batch 4:  0.6392\n",
      "Loss: 0.5726 Validation Accuracy: 0.639200\n",
      "Epoch 182, CIFAR-10 Batch 5:  0.6508\n",
      "Loss: 0.5791 Validation Accuracy: 0.650800\n",
      "Epoch 183, CIFAR-10 Batch 1:  0.6438\n",
      "Loss: 0.6136 Validation Accuracy: 0.643800\n",
      "Epoch 183, CIFAR-10 Batch 2:  0.652\n",
      "Loss: 0.6103 Validation Accuracy: 0.652000\n",
      "Epoch 183, CIFAR-10 Batch 3:  0.6546\n",
      "Loss: 0.5275 Validation Accuracy: 0.654600\n",
      "Epoch 183, CIFAR-10 Batch 4:  0.6384\n",
      "Loss: 0.5771 Validation Accuracy: 0.638400\n",
      "Epoch 183, CIFAR-10 Batch 5:  0.6484\n",
      "Loss: 0.5748 Validation Accuracy: 0.648400\n",
      "Epoch 184, CIFAR-10 Batch 1:  0.6512\n",
      "Loss: 0.6087 Validation Accuracy: 0.651200\n",
      "Epoch 184, CIFAR-10 Batch 2:  0.6494\n",
      "Loss: 0.5999 Validation Accuracy: 0.649400\n",
      "Epoch 184, CIFAR-10 Batch 3:  0.6466\n",
      "Loss: 0.5401 Validation Accuracy: 0.646600\n",
      "Epoch 184, CIFAR-10 Batch 4:  0.6382\n",
      "Loss: 0.5968 Validation Accuracy: 0.638200\n",
      "Epoch 184, CIFAR-10 Batch 5:  0.6414\n",
      "Loss: 0.5843 Validation Accuracy: 0.641400\n",
      "Epoch 185, CIFAR-10 Batch 1:  0.6454\n",
      "Loss: 0.6126 Validation Accuracy: 0.645400\n",
      "Epoch 185, CIFAR-10 Batch 2:  0.6484\n",
      "Loss: 0.5825 Validation Accuracy: 0.648400\n",
      "Epoch 185, CIFAR-10 Batch 3:  0.6536\n",
      "Loss: 0.5169 Validation Accuracy: 0.653600\n",
      "Epoch 185, CIFAR-10 Batch 4:  0.64\n",
      "Loss: 0.5618 Validation Accuracy: 0.640000\n",
      "Epoch 185, CIFAR-10 Batch 5:  0.638\n",
      "Loss: 0.5917 Validation Accuracy: 0.638000\n",
      "Epoch 186, CIFAR-10 Batch 1:  0.6504\n",
      "Loss: 0.6162 Validation Accuracy: 0.650400\n",
      "Epoch 186, CIFAR-10 Batch 2:  0.6508\n",
      "Loss: 0.6122 Validation Accuracy: 0.650800\n",
      "Epoch 186, CIFAR-10 Batch 3:  0.6534\n",
      "Loss: 0.5473 Validation Accuracy: 0.653400\n",
      "Epoch 186, CIFAR-10 Batch 4:  0.6416\n",
      "Loss: 0.5597 Validation Accuracy: 0.641600\n",
      "Epoch 186, CIFAR-10 Batch 5:  0.6486\n",
      "Loss: 0.5845 Validation Accuracy: 0.648600\n",
      "Epoch 187, CIFAR-10 Batch 1:  0.6506\n",
      "Loss: 0.6099 Validation Accuracy: 0.650600\n",
      "Epoch 187, CIFAR-10 Batch 2:  0.6464\n",
      "Loss: 0.5847 Validation Accuracy: 0.646400\n",
      "Epoch 187, CIFAR-10 Batch 3:  0.657\n",
      "Loss: 0.5239 Validation Accuracy: 0.657000\n",
      "Epoch 187, CIFAR-10 Batch 4:  0.6298\n",
      "Loss: 0.5899 Validation Accuracy: 0.629800\n",
      "Epoch 187, CIFAR-10 Batch 5:  0.645\n",
      "Loss: 0.5735 Validation Accuracy: 0.645000\n",
      "Epoch 188, CIFAR-10 Batch 1:  0.6582\n",
      "Loss: 0.6146 Validation Accuracy: 0.658200\n",
      "Epoch 188, CIFAR-10 Batch 2:  0.6576\n",
      "Loss: 0.5974 Validation Accuracy: 0.657600\n",
      "Epoch 188, CIFAR-10 Batch 3:  0.6458\n",
      "Loss: 0.5279 Validation Accuracy: 0.645800\n",
      "Epoch 188, CIFAR-10 Batch 4:  0.6382\n",
      "Loss: 0.5648 Validation Accuracy: 0.638200\n",
      "Epoch 188, CIFAR-10 Batch 5:  0.6472\n",
      "Loss: 0.5504 Validation Accuracy: 0.647200\n",
      "Epoch 189, CIFAR-10 Batch 1:  0.648\n",
      "Loss: 0.6015 Validation Accuracy: 0.648000\n",
      "Epoch 189, CIFAR-10 Batch 2:  0.6478\n",
      "Loss: 0.5863 Validation Accuracy: 0.647800\n",
      "Epoch 189, CIFAR-10 Batch 3:  0.653\n",
      "Loss: 0.5211 Validation Accuracy: 0.653000\n",
      "Epoch 189, CIFAR-10 Batch 4:  0.6434\n",
      "Loss: 0.5638 Validation Accuracy: 0.643400\n",
      "Epoch 189, CIFAR-10 Batch 5:  0.648\n",
      "Loss: 0.5716 Validation Accuracy: 0.648000\n",
      "Epoch 190, CIFAR-10 Batch 1:  0.6408\n",
      "Loss: 0.6243 Validation Accuracy: 0.640800\n",
      "Epoch 190, CIFAR-10 Batch 2:  0.6406\n",
      "Loss: 0.5805 Validation Accuracy: 0.640600\n",
      "Epoch 190, CIFAR-10 Batch 3:  0.6514\n",
      "Loss: 0.5332 Validation Accuracy: 0.651400\n",
      "Epoch 190, CIFAR-10 Batch 4:  0.643\n",
      "Loss: 0.5532 Validation Accuracy: 0.643000\n",
      "Epoch 190, CIFAR-10 Batch 5:  0.6462\n",
      "Loss: 0.5843 Validation Accuracy: 0.646200\n",
      "Epoch 191, CIFAR-10 Batch 1:  0.6556\n",
      "Loss: 0.6121 Validation Accuracy: 0.655600\n",
      "Epoch 191, CIFAR-10 Batch 2:  0.6492\n",
      "Loss: 0.5917 Validation Accuracy: 0.649200\n",
      "Epoch 191, CIFAR-10 Batch 3:  0.6536\n",
      "Loss: 0.5125 Validation Accuracy: 0.653600\n",
      "Epoch 191, CIFAR-10 Batch 4:  0.637\n",
      "Loss: 0.5649 Validation Accuracy: 0.637000\n",
      "Epoch 191, CIFAR-10 Batch 5:  0.6458\n",
      "Loss: 0.5790 Validation Accuracy: 0.645800\n",
      "Epoch 192, CIFAR-10 Batch 1:  0.6562\n",
      "Loss: 0.6088 Validation Accuracy: 0.656200\n",
      "Epoch 192, CIFAR-10 Batch 2:  0.6532\n",
      "Loss: 0.5987 Validation Accuracy: 0.653200\n",
      "Epoch 192, CIFAR-10 Batch 3:  0.654\n",
      "Loss: 0.5210 Validation Accuracy: 0.654000\n",
      "Epoch 192, CIFAR-10 Batch 4:  0.6372\n",
      "Loss: 0.5551 Validation Accuracy: 0.637200\n",
      "Epoch 192, CIFAR-10 Batch 5:  0.6474\n",
      "Loss: 0.5622 Validation Accuracy: 0.647400\n",
      "Epoch 193, CIFAR-10 Batch 1:  0.6486\n",
      "Loss: 0.6124 Validation Accuracy: 0.648600\n",
      "Epoch 193, CIFAR-10 Batch 2:  0.6472\n",
      "Loss: 0.5993 Validation Accuracy: 0.647200\n",
      "Epoch 193, CIFAR-10 Batch 3:  0.6502\n",
      "Loss: 0.5155 Validation Accuracy: 0.650200\n",
      "Epoch 193, CIFAR-10 Batch 4:  0.649\n",
      "Loss: 0.5462 Validation Accuracy: 0.649000\n",
      "Epoch 193, CIFAR-10 Batch 5:  0.65\n",
      "Loss: 0.5677 Validation Accuracy: 0.650000\n",
      "Epoch 194, CIFAR-10 Batch 1:  0.6534\n",
      "Loss: 0.6336 Validation Accuracy: 0.653400\n",
      "Epoch 194, CIFAR-10 Batch 2:  0.659\n",
      "Loss: 0.5926 Validation Accuracy: 0.659000\n",
      "Epoch 194, CIFAR-10 Batch 3:  0.6526\n",
      "Loss: 0.5037 Validation Accuracy: 0.652600\n",
      "Epoch 194, CIFAR-10 Batch 4:  0.6326\n",
      "Loss: 0.5543 Validation Accuracy: 0.632600\n",
      "Epoch 194, CIFAR-10 Batch 5:  0.6546\n",
      "Loss: 0.5542 Validation Accuracy: 0.654600\n",
      "Epoch 195, CIFAR-10 Batch 1:  0.6514\n",
      "Loss: 0.6095 Validation Accuracy: 0.651400\n",
      "Epoch 195, CIFAR-10 Batch 2:  0.6432\n",
      "Loss: 0.5922 Validation Accuracy: 0.643200\n",
      "Epoch 195, CIFAR-10 Batch 3:  0.6502\n",
      "Loss: 0.5205 Validation Accuracy: 0.650200\n",
      "Epoch 195, CIFAR-10 Batch 4:  0.644\n",
      "Loss: 0.5570 Validation Accuracy: 0.644000\n",
      "Epoch 195, CIFAR-10 Batch 5:  0.6478\n",
      "Loss: 0.5761 Validation Accuracy: 0.647800\n",
      "Epoch 196, CIFAR-10 Batch 1:  0.6564\n",
      "Loss: 0.6027 Validation Accuracy: 0.656400\n",
      "Epoch 196, CIFAR-10 Batch 2:  0.6472\n",
      "Loss: 0.6018 Validation Accuracy: 0.647200\n",
      "Epoch 196, CIFAR-10 Batch 3:  0.645\n",
      "Loss: 0.5247 Validation Accuracy: 0.645000\n",
      "Epoch 196, CIFAR-10 Batch 4:  0.6336\n",
      "Loss: 0.5697 Validation Accuracy: 0.633600\n",
      "Epoch 196, CIFAR-10 Batch 5:  0.65\n",
      "Loss: 0.5614 Validation Accuracy: 0.650000\n",
      "Epoch 197, CIFAR-10 Batch 1:  0.6494\n",
      "Loss: 0.6136 Validation Accuracy: 0.649400\n",
      "Epoch 197, CIFAR-10 Batch 2:  0.6564\n",
      "Loss: 0.5913 Validation Accuracy: 0.656400\n",
      "Epoch 197, CIFAR-10 Batch 3:  0.6552\n",
      "Loss: 0.5162 Validation Accuracy: 0.655200\n",
      "Epoch 197, CIFAR-10 Batch 4:  0.6502\n",
      "Loss: 0.5497 Validation Accuracy: 0.650200\n",
      "Epoch 197, CIFAR-10 Batch 5:  0.6402\n",
      "Loss: 0.5680 Validation Accuracy: 0.640200\n",
      "Epoch 198, CIFAR-10 Batch 1:  0.6534\n",
      "Loss: 0.6098 Validation Accuracy: 0.653400\n",
      "Epoch 198, CIFAR-10 Batch 2:  0.6576\n",
      "Loss: 0.5717 Validation Accuracy: 0.657600\n",
      "Epoch 198, CIFAR-10 Batch 3:  0.6538\n",
      "Loss: 0.5123 Validation Accuracy: 0.653800\n",
      "Epoch 198, CIFAR-10 Batch 4:  0.6426\n",
      "Loss: 0.5404 Validation Accuracy: 0.642600\n",
      "Epoch 198, CIFAR-10 Batch 5:  0.6406\n",
      "Loss: 0.5402 Validation Accuracy: 0.640600\n",
      "Epoch 199, CIFAR-10 Batch 1:  0.6416\n",
      "Loss: 0.6004 Validation Accuracy: 0.641600\n",
      "Epoch 199, CIFAR-10 Batch 2:  0.6468\n",
      "Loss: 0.5872 Validation Accuracy: 0.646800\n",
      "Epoch 199, CIFAR-10 Batch 3:  0.6522\n",
      "Loss: 0.5218 Validation Accuracy: 0.652200\n",
      "Epoch 199, CIFAR-10 Batch 4:  0.6218\n",
      "Loss: 0.5687 Validation Accuracy: 0.621800\n",
      "Epoch 199, CIFAR-10 Batch 5:  0.6316\n",
      "Loss: 0.5783 Validation Accuracy: 0.631600\n",
      "Epoch 200, CIFAR-10 Batch 1:  0.6514\n",
      "Loss: 0.5849 Validation Accuracy: 0.651400\n",
      "Epoch 200, CIFAR-10 Batch 2:  0.6504\n",
      "Loss: 0.6006 Validation Accuracy: 0.650400\n",
      "Epoch 200, CIFAR-10 Batch 3:  0.6516\n",
      "Loss: 0.5042 Validation Accuracy: 0.651600\n",
      "Epoch 200, CIFAR-10 Batch 4:  0.6402\n",
      "Loss: 0.5504 Validation Accuracy: 0.640200\n",
      "Epoch 200, CIFAR-10 Batch 5:  0.6482\n",
      "Loss: 0.5751 Validation Accuracy: 0.648200\n",
      "CPU times: user 11min 4s, sys: 5min 14s, total: 16min 18s\n",
      "Wall time: 17min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "8min 56s            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.6424115359783172\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/x/rU8stVsVa7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f881c7e44a8>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
